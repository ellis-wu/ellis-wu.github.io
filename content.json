{"meta":{"title":"Ellis's Blog","subtitle":null,"description":null,"author":"Ellis Wu","url":"http://ellis-wu.github.io"},"pages":[{"title":"About","date":"2019-07-22T02:21:17.799Z","updated":"2019-07-22T02:21:17.799Z","comments":true,"path":"about/index.html","permalink":"http://ellis-wu.github.io/about/index.html","excerpt":"","text":"我是誰 大家好！我是豐名(Ellis)，目前在公司擔任軟體工程師，為一個小小的研發替代役兼菜鳥。在學期間與工作期間接觸與研究許多領域。因此，將所接觸與研究的技術或非技術記錄下來，分享給大家或者方便之後自己複習，若有任何問題與指教亦請大家不吝指教。 經歷 就讀大學期間，主要撰寫 Objective-C 程式語言，專注於開發 iOS Mobile 應用程式。而在學期間參與許多比賽以及產學合作，學習與企業如何 Co-work 完成一項專案之開發。而合作企業行跨不同領域，如：旅遊產業、雲端產業以及健身器材產業等等。 而在研究所期間，從終端的研究轉為雲的研究，開始接觸並研究雲端開源軟體 OpenStack、分散式檔案儲存系統 Ceph 、輕量級虛擬化 Docker 以及容器管理與調度工具 Kubernetes 等。 期許 賈伯斯曾說過： 虛懷若谷；求知若渴 學習是無止境的，抱著謙卑的心態學習任何知識，並將其紀錄下來。因此，這裡大多會紀錄雲原生(Cloud Native)相關技術，若有任何問題歡迎提出討論。"},{"title":"Categories","date":"2019-07-22T02:21:17.799Z","updated":"2019-07-22T02:21:17.799Z","comments":true,"path":"categories/index.html","permalink":"http://ellis-wu.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2019-07-22T02:21:17.845Z","updated":"2019-07-22T02:21:17.845Z","comments":true,"path":"tags/index.html","permalink":"http://ellis-wu.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Alexa Skill 結合 Auth0","slug":"ask-with-auth0","date":"2019-07-21T16:00:00.000Z","updated":"2019-08-20T02:44:43.323Z","comments":true,"path":"2019/07/22/ask-with-auth0/","link":"","permalink":"http://ellis-wu.github.io/2019/07/22/ask-with-auth0/","excerpt":"在上一篇Alexa Skills Kit 新手教學介紹了怎麼建立一個簡單的 Alexa Skill 的範例。若有考慮到要將 Skill 產品化應該都會遇到一些問題，就是當我對 Echo 或 APP 說了一指令希望它去打開臥室的門，它要怎麼知道該使用者的臥室是哪個裝置進而讓它去執行開門的動作呢？這時候我們需要借助Account Linking這個功能，這是在使用者啟用這個技能時，需要使用者授權讓該技能擁有存取使用者資訊的功能。當我們能存取使用者的一些資訊後，就可以去查詢使用者的臥室是哪個裝置，進而去控制裝置的動作。而Account Linking是使用OAuth 2.0協定來進行的，你當然可以選擇自己建立 OAuth Server 或者使用其他第三方服務。而本文將使用 Auth0 結合 Alexa Skill。","text":"在上一篇Alexa Skills Kit 新手教學介紹了怎麼建立一個簡單的 Alexa Skill 的範例。若有考慮到要將 Skill 產品化應該都會遇到一些問題，就是當我對 Echo 或 APP 說了一指令希望它去打開臥室的門，它要怎麼知道該使用者的臥室是哪個裝置進而讓它去執行開門的動作呢？這時候我們需要借助Account Linking這個功能，這是在使用者啟用這個技能時，需要使用者授權讓該技能擁有存取使用者資訊的功能。當我們能存取使用者的一些資訊後，就可以去查詢使用者的臥室是哪個裝置，進而去控制裝置的動作。而Account Linking是使用OAuth 2.0協定來進行的，你當然可以選擇自己建立 OAuth Server 或者使用其他第三方服務。而本文將使用 Auth0 結合 Alexa Skill。 使用流程與架構使用者若要使用客製化技能，是有些前置動作要完成的。使用者在使用客製化技能前，需要在 Alexa APP 或網頁來啟用該技能，而我們的 skill 會設定必須進行 account linking。所以當使用者在 APP 或網頁點擊 [Enable] 後，會彈出一個登入/註冊網站，當使用者輸入資料後並點擊登入/註冊按鈕後，會將該資料儲存至 Auth0 之中。成功後，使用者就會在 APP 或網頁看到該 skill 已經成功啟用且連結了。 前置步驟完成，我們可以來看看當使用者使用客製化技能的流程。當使用者說出客製化技能的指令，一樣會將語音串流傳送至 AVS，而 AVS 將語音轉換為文字再進行分析，分析後則觸發該技能的 Lambda 函式。而 Lambda 的 Intent Handler 觸發後，會先跟 Auth0 取得該使用者的基本資料(不包含 Device UUID)。再從取得的基本資料拿到該使用者在 Auth0 中的 ID，我們利用該 ID 從 Auth0 提供的 Management API 查詢該使用者的詳細資訊。最後再從使用者的詳細資訊取得它的 Device UUID。接著可以利用這些資訊往第三方服務(MQTT 等服務)傳送，告知該 Device UUID 該執行動作了。而使用者家中的裝置接收到後則作出對應的動作。 設定 Auth0要設定 Auth0 當然要先擁有 Auth0 的帳號。請先自行註冊 Auth0 帳號後再進行以下步驟。 建立與設定 Auth0 Application登入後，點擊 [CREATE APPLICATION] 先創建一個 application。 接著，輸入 application 的 [Name] 並且選擇類型為 [Machine to Machine Applications]。 輸入完成後，點擊 [CREATE]。 接著會要選則這個 application 他能存取與使用的 API。下拉式選單選擇 [Auth0 Management API] 後，再將 scopes 選擇 [all]。 本文是範例教學，所以將所有 API 權限打開。請依照自己的使用情境選擇要開起來些 API 權限。 選擇完畢後，點擊 [AUTHORIZE]。完成後會看到類似以下畫面。 接下來，要對這個 application 設定一些配置。切換上方 tab 至 Settings，找到 Token Endpoint Authentication Method 並設定成 [Basic]。 再來，在設定頁面最下方，會看到 [Show Advances Settings] 點擊並將 tab 切換至 OAuth，再來找到 JsonWebToken Signature Algorithm 欄位並選擇 [HS256]。 完成後，點擊下方 [SAVE CHANGE] 按鈕。 (Options) Social Logins這是使用者在登入時，可以選擇使用其他第三方帳號(Facebook\b、Google 與 Amazon 等等)的方式登入。每個第三方帳號的設定都不盡相同，這邊就看自己的需求自行設定。而本文這邊就不進行說明，若想要設定請參考這裏。 設定 Alexa Skill這邊要開始 Alexa Skill 與 Auth0 之間的設定。我們先打開自己建立的 Alexa Skill 頁面，並點選左邊的 Account Linking。 如上圖所示，將 Do you allow users to create an account or link to an existing account with you? 的開關打開。而 Allow users to enable skill without account linking 選項則是看個人需求，如果選擇關閉；則在啟用這個技能時就會直接請求 Account Linking。 接來來，在 Authorization Grant Type 選項選擇 [Auth Code Grant]。選擇後，會看到下方有好幾項設定，而這些設定都可以從 Auth0 那邊取得： Authorization URL：在剛剛於 Auth0 建立的 application 中的 [Settings] → [Advanced Settings] → [Endpoints] → [OAuth Authorization URL]。 Access Token URI：在剛剛於 Auth0 建立的 application 中的 [Settings] → [Advanced Settings] → [Endpoints] → [OAuth Token URL]。 Client ID：在剛剛於 Auth0 建立的 application 中的 [Settings] → [Client ID]。 Client Secret在剛剛於 Auth0 建立的 application 中的 [Settings] → [Client Secret]。 Client Authentication Scheme：依然選擇 [HTTP Basic]。 Scope：分別新增並輸入openid、offline_access、profile與email。 Domain List：新增並輸入cdn.auth0.com。 以上資訊輸入完成後，就可以按下上方 [Save] 按鈕。而輸入完資訊類似下面這樣。 接下來，請記住 Alexa Skill Account Linking 下方的 Redirect URLs 的三個網址。我們要將它設定至 Auth0 上。這是當 Auth0 成功授權登入後，會重新導回這個 URL。在剛剛於 Auth0 建立的 application 中的 [Settings] 找到 Allowed Callback URLs 欄位，輸入剛剛記下來的三個 Redirect URLs 並以逗號區隔。 輸入完後，請得點選下方的 [SAVE CHANGE] 按鈕。 第一次測試我們透過 Amazon Alexa 網頁版進行測試，當然也可以從 Alexa APP 進行測試，但在台灣是下載不到 Alexa APP 的，所以要切換至別的國家才有辦法下載。 登入後，我們點選左邊導航列的 Skills，再點選右上的 Your Skills，可以查詢你所擁有的 Skill。這邊可以看到 Enable Skill 會看到自己建立的客製化技能。 可以看到這個 skill 是 Enable 的狀態且下方顯示 Account linking required，這代表我們 account linking 是有啟用的狀態。我們可以點選旁邊的 SETTINGS 按鈕。點擊後我們會看到類似以下畫面，這邊可以進行 account linking 的動作。 【重要觀念】Alexa Skill 的啟用與 account linking 是分開的，意思是可以單獨啟用該技能卻不進行 account linking。若不進行 account linking 的話，該使用者還是能使用該 skill，但在 lambda 則無法取得該使用者的access token 來取得該使用者的個人資料。 點擊右邊小小的 [Link Account] 按鈕，就會轉跳至 Auth0 提供的登入畫面，如下圖所示。這時候我們先暫時不作任何動作，因為我們需要更改一下 Auth0 登入頁面讓使用者在進行 account linking 時輸入裝置的 UUID，以及修改 skill 的 Lambda 程式讓使用者使用客製化技能時，能確認是否已經進行 account linking，若已經進行 account linking 則能取得使用者的裝置 UUID。 修改 Auth0 Login 畫面真的不得不說 Auth0 整合得很完善也很方便，它提供了Universal Login讓開發者能夠快速建立登入授權驗證網頁。而Universal Login它提供了三種選擇： Lock Lock (passwordless) Custom Login Form 其中 Lock 與 Custom Login Form 之間最大的差異是 Lock 都是透過 Auth0 提供的 library 來建立登入授權驗證網頁，其它提供了登入授權驗證網頁的樣板，因此畫面較為固定，若需在網頁上新增欄位讓使用者填寫的話，則要依照它的方式來新增欄位。而 Custom Login Form 則是登入授權驗證網頁的 html 與 css 都可以自行修改，而透過script載入 Auth0 的函式讓開發者能進行登入與註冊的動作。 這邊我們使用 Custom Login Form 來建立自己的登入授權驗證網頁。首先，到 Auth0 Dashboard 點選左邊的 Universal Login，然後切換中間 tab 至 Login，再來將中間 Customize Login Page 的開關開啟，再將下方的 DEFAULT TEMPLATES 下拉式選單選擇至 [Custom Login Form]。 選擇完後，會看到程式碼與一開始不同，接著我們稍微來修改一下程式碼。先加入一個 Device UUID 的輸入框：... &lt;div class=\"form-group\"&gt; &lt;label for=\"name\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"password\" placeholder=\"Enter your password\"&gt; &lt;/div&gt; &lt;!-- 加入下面程式 \b--&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"name\"&gt;Device UUID&lt;/label&gt; &lt;input type=\"deviceUUID\" class=\"form-control\" id=\"deviceUUID\" placeholder=\"Enter your device UUID\"&gt; &lt;/div&gt;... 再來是在註冊時，我們要將 Device UUID 儲存至 Auth0 中。而 Auth0 在註冊時，我們可以將 Device UUID 存入至使用者的user_metadata中，之後再透過 API 將其取出。所以，修改程式碼中signup函式：... function signup() &#123; var email = document.getElementById('email').value; var password = document.getElementById('password').value; var deviceUUID = document.getElementById('deviceUUID').value; webAuth.redirect.signupAndLogin(&#123; connection: databaseConnection, email: email, password: password, user_metadata: &#123; deviceUUID: deviceUUID, &#125; &#125;, function(err) &#123; if (err) displayError(err); &#125;); &#125;... 完成後，可以點擊上面的 [Preview] 按鈕來看一下目前的登入授權驗證網頁。 確認沒問題後，就可以按下方的 [SAVE CHANGES] 按鈕來儲存。 修改 Lambda 程式碼當使用者說出「Alexa, tell myskill open the door」時，會發客製化技能的 Lambda 執行。而這時候，在 Lambda 中要去抓取該使用者的 Device UUID 並往第三方服務發送一個 action 的訊息，而客戶的裝置為該 Device UUID 則執行動作。 為了達到上述的情況，我們需要修改 Alexa Skill 上 Lambda 的程式碼。請參考下方的程式碼進行修改，其中有些參數請更改為自己的資訊。我們可以從 Lambda 觸發時的 handler 取得access token(只有成功 account linking 時才能取得到)，再透過該access token呼叫 Auth0 的 userinfo API 取得使用者的資訊，但 Response 中的使用者資訊並不包含我們儲存device UUID的user_metadata。這是因為 userinfo API 取得的資料格式必須遵照OpenID，而user_metadata並不屬於OpenID的 scope。 因此，我們利用剛剛從 userinfo API 取得到的 user ID，再透過 Auth0 的 Management API 來查詢該使用者的資訊，並取得user_metadata的 device UUID。在使用 Management API 查詢使用者之前，需要先向 Auht0 取得 token 來存取 Management，否則會無法存取。const DoorIntentHandler = &#123; canHandle(handlerInput) &#123; return handlerInput.requestEnvelope.request.type === 'IntentRequest' &amp;&amp; handlerInput.requestEnvelope.request.intent.name === 'HelloWorldIntent'; &#125;, async handle(handlerInput) &#123; // Get user access token const &#123; accessToken &#125; = handlerInput.requestEnvelope.context.System.user; let speechText = ''; if (!accessToken) &#123; // If didn't have access token then ask user link account speechText = 'Please link your account'; return handlerInput.responseBuilder.speak(speechText).getResponse(); &#125; else &#123; // Using userinfo API to get user informations with OpenID format let userInfoOptions = &#123; headers: &#123; authorization: 'Bearer ' + accessToken &#125; &#125;; const userInfoResponse = await axios.get('https://&#123;your auth0 application url&#125;/userinfo', userInfoOptions); const userID = userInfoResponse.data.sub; // Get the user UUID // Get token for search the user informations to get user's device UUID let tokenPayload = &#123; 'client_id': '&#123;your client ID&#125;', 'client_secret': '&#123;your client secret&#125;', 'audience': 'https://&#123;your auth0 application url&#125;/api/v2/', 'grant_type': 'client_credentials' &#125;; let tokenHeaders = &#123; headers: &#123; 'content-type': 'application/json' &#125; &#125;; const tokenResponse = await axios.post('https://&#123;your auth0 application url&#125;/oauth/token', tokenPayload, tokenHeaders); const token = tokenResponse.data.access_token; // Get device UUID in user informations via Auth0 Management API let deviceHeader = &#123; headers: &#123; authorization: 'Bearer ' + token &#125; &#125;; const deviceResponse = await axios.get(`https://&#123;your auth0 application url&#125;/api/v2/users/$&#123;userID&#125;`, deviceHeader); const deviceID = deviceResponse.data.user_metadata.deviceUUID; // Do something in there // ... speechText = `Open $&#123;deviceID&#125; Door`; return handlerInput.responseBuilder.speak(speechText).reprompt(speechText).getResponse(); &#125; &#125;&#125;; 因為我們有用axios來呼叫 API，所以先在程式碼最上方加入這行：const axios = require('axios'); 接著，在package.json中加入axios：&#123; \"name\": \"hello-world\", \"version\": \"1.1.0\", \"description\": \"alexa utility for quickly building skills\", \"main\": \"index.js\", \"scripts\": &#123; \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\" &#125;, \"author\": \"Amazon Alexa\", \"license\": \"ISC\", \"dependencies\": &#123; \"ask-sdk-core\": \"^2.6.0\", \"ask-sdk-model\": \"^1.18.0\", \"aws-sdk\": \"^2.326.0\", \"axios\": \"^0.18.0\" &#125;&#125; 完成後，點擊上方 [Save] 並且 [Deploy]。 開始測試！部署完成後，依樣可以切換上方的導航列至 [Test]。首先，我們先測試看看若未進行 account linking，是否會回覆我們 “Please link your account”。 接著使用 Alexa APP 或網站進行 account linking 來測試看看結果如何。點擊 [Link Account] 按鈕。 點擊後，會轉跳至我們剛剛設定的登入授權驗證網頁，並輸入帳號密碼與裝置的 UUID。輸入完成後按下 [Sign Up] 按鈕。 完成後，若成功會看到該技能已經成功連結。 我們可以到 Auth0 的 Dashboard 查看剛剛註冊的使用者，且可以看到user_metadata有儲存剛剛輸入的deviceUUID。 再來進行 account linking 後在測試一次看看結果。 這樣就大功告成拉！","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"AWS","slug":"技術/AWS","permalink":"http://ellis-wu.github.io/categories/技術/AWS/"}],"tags":[{"name":"Amazon","slug":"Amazon","permalink":"http://ellis-wu.github.io/tags/Amazon/"},{"name":"Alexa","slug":"Alexa","permalink":"http://ellis-wu.github.io/tags/Alexa/"},{"name":"Auth0","slug":"Auth0","permalink":"http://ellis-wu.github.io/tags/Auth0/"}]},{"title":"Alexa Skills Kit 新手教學","slug":"alexa-skills-kit","date":"2019-07-18T16:00:00.000Z","updated":"2019-08-16T06:14:25.989Z","comments":true,"path":"2019/07/19/alexa-skills-kit/","link":"","permalink":"http://ellis-wu.github.io/2019/07/19/alexa-skills-kit/","excerpt":"Amazon 於 2014 年推出了 Alexa 智慧語音助手，就目前來說 Alexa 應該已經是家喻戶曉的東西了。透過 Amazon Echo 搭配 Alexa 就能讓冷冰冰的機器能與使用者對話，而要做到這樣的事情在其背後是需要龐大的架構來支撐的。在整個 Alexa 生態系中有兩個重要的東西，第一個是 Alexa Voice Service (AVS)，它是讓機器與人溝通重要的中樞神經，所有接收到的語音檔案都會傳至 AVS 加以解析並判斷，再回覆給使用者。第二個是 Alexa Skills Kit (ASK)，它是讓人能有客製化的語音命令，透過 ASK 能控制設備或更多的功能 (像是訂購 pizza 等)，藉由 ASK 來強壯整個 Alexa 的生態系。所以，本文會先簡單介紹 AVS 與 ASK，在以一個簡單的教學說明如何客製化一個自己的技能。","text":"Amazon 於 2014 年推出了 Alexa 智慧語音助手，就目前來說 Alexa 應該已經是家喻戶曉的東西了。透過 Amazon Echo 搭配 Alexa 就能讓冷冰冰的機器能與使用者對話，而要做到這樣的事情在其背後是需要龐大的架構來支撐的。在整個 Alexa 生態系中有兩個重要的東西，第一個是 Alexa Voice Service (AVS)，它是讓機器與人溝通重要的中樞神經，所有接收到的語音檔案都會傳至 AVS 加以解析並判斷，再回覆給使用者。第二個是 Alexa Skills Kit (ASK)，它是讓人能有客製化的語音命令，透過 ASK 能控制設備或更多的功能 (像是訂購 pizza 等)，藉由 ASK 來強壯整個 Alexa 的生態系。所以，本文會先簡單介紹 AVS 與 ASK，在以一個簡單的教學說明如何客製化一個自己的技能。 什麼是 AVS？首先，我們要先了解何謂 Alexa Voice Service (AVS)，這是使用者每對 Echo 說話時，都一定會用到的服務。如下圖所示，Echo 不斷的偵測，並且在接收到喚醒詞 Alexa 後，開始將喚醒詞後面的語音 What time is it 傳送至 AVS。而 AVS 接收到語音串流後，會將語音轉文字，並加以分析該語音的意思，最終 AVS 將文字結果回覆給 Echo 並由 Echo 將文字給說出來。 ASK 與 AVS可能大家會開始思考，那我要控制家裡的家電 Alexa 是又如何能明白家中的裝置與開關。這是 Amazon 提供了 ASK 讓大家能客製化自己的語音命令以達到控制家電或者查詢特定資料等功能。 如上圖所示。是一個客製化技能與 AVS 之間的流程。假設使用者對家中 Echo 說「Alexa, tell Myskill Open door」，其語音串流與資料流如下： Echo 不停偵測 Alexa 這個喚醒詞，當接收到後喚醒 Alexa 將喚醒詞後面語音串流傳送至 AVS。 AVS 將語音串流進行辨識並將其轉為文字，轉文字後再將其切割成要被激活的應用Invocation (此例為 Myskill) 以及其他已被辨識為文字的命令句Intent (此例為 Open door)。 AVS 根據Invocation找到對應的客製化技能內所設定的範例詞彙 (sample utterances) 進行比對，若符合則觸發該Intent。 AWS Lambda 會事先定義好各個Intent被觸發時要執行的事件，所以步驟 3 觸發的Intent就會執行 Lambda 中對應的事情。 最後將處理結果與回覆的語音等等回傳至 AVS，而其中語音部分會被 Echo 播出。 以上大概是一個客製化技能的流程。而要出發該客製化技能有對話形式與命令形式兩種形式，差異如下圖。 有了一些基本的了解後，讓我們開始建立一個自己的 Alexa 客製化技能吧! 建立自己的 Alexa Skill首先，打開瀏覽器並存取 Alexa 網頁。登入後，上方導航列選擇 [Alexa] 再選擇 [Alexa Skill Kit] 進入 Alexa Skills Kit (ASK) 的 console 頁面。 接著，我們點選右邊的 [Create Skill] 按鈕來建立自己的 Alexa Skill。 第一件事情當然是要輸入 Skill 的名稱，而這個名稱最後會是你在 Alexa Store 上的名稱。 選擇 Alexa Skill 的模板，每個模板在設定與使用上都有不同，所以這邊請依照自己情境選擇最合適的。而這邊為範例所以選擇 [Custom]。 Alexa Skill 都需要一個 backend，讓語音命令執行時告訴它應該要做什麼事情。而目前 Alexa 提供了 Provision your own 與 Alexa-Hosted 兩種方式。而這邊為範例所以選擇 [Alexa-Hosted]。 確認資訊都沒問題後，就可以點選上方的 [Create Skill] 按鈕。而這邊需要等它一下，讓它建立聲音模型與建立 backend 等事情。 完成後，就會看到類似以下的畫面。 可以看到畫面的左邊的 Interaction Model，這是整個 Skill 最重要部分，Alexa Skill 的觸發與執行的命令都會在這邊。因此，我們先來設定 Alexa Skill 觸發的語音，點選左邊的 [Invocation]。可以看到它都有一些說明，若設定的 Skill Invocation Name 為 myskill 時，使用者對家中的 Alexa 設備說 “Alexa, ask myskill balalala…”，這樣才能觸發這個技能並叫它做對應的事情。 輸入完 [Skill Invocation Name] 後，記得點選上方的 [Sava Model]。 接下來要設定執行的語音指令。這邊我們自己建立一個新的 Intent。點選左邊 Intent 的 [Add] 按鈕。輸入 [Create custom intent] 的名稱。輸入完後，點選 [Create custom intent] 按鈕。 若當初建立 Skill 時選擇 Alexa-Hosted 這邊會幫你自己建立HelloWorldIntent。 再來設定這個 Inent 的 Sample Utterances 來觸發這個 Intent。 若如上圖設定，則代表我們說出的命令如下： Alexa, ask myskill open the door Alexa, ask myskill open door Alexa, ask myskill door open 設定完成後一樣記得點選上方的 [Save Model]。到這邊我們基本的設定已大致完成，這時候點選旁邊的 [Build Model] 來建立 Invocation 與 Intent 的模型。 訓練完成後，要對新加入的 Intent 來寫一點程式讓它觸發這個 Intent 時要回覆語音。在上方導航列點選 [Code]。 若當初有選 [Alexa-Hosted]，在 Code 這邊才會自己建立 Lambda 程式。若沒有選擇，則要自己透過 Amazon Lambda 來建立程式碼，並將 Lambda 與這個 Skill 連結。 Code 中的 Lambda 程式如下，這邊可以很清楚看到程式碼定義了每個 request 與 intent 要做的事情或回覆的語音。// This sample demonstrates handling intents from an Alexa skill using the Alexa Skills Kit SDK (v2).// Please visit https://alexa.design/cookbook for additional examples on implementing slots, dialog management,// session persistence, api calls, and more.const Alexa = require('ask-sdk-core');const LaunchRequestHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'LaunchRequest'; &#125;, handle(handlerInput) &#123; const speakOutput = 'Welcome, you can say Hello or Help. Which would you like to try?'; return handlerInput.responseBuilder .speak(speakOutput) .reprompt(speakOutput) .getResponse(); &#125;&#125;;const HelloWorldIntentHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest' &amp;&amp; Alexa.getIntentName(handlerInput.requestEnvelope) === 'HelloWorldIntent'; &#125;, handle(handlerInput) &#123; const speakOutput = 'Hello World!'; return handlerInput.responseBuilder .speak(speakOutput) //.reprompt('add a reprompt if you want to keep the session open for the user to respond') .getResponse(); &#125;&#125;;const HelpIntentHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest' &amp;&amp; Alexa.getIntentName(handlerInput.requestEnvelope) === 'AMAZON.HelpIntent'; &#125;, handle(handlerInput) &#123; const speakOutput = 'You can say hello to me! How can I help?'; return handlerInput.responseBuilder .speak(speakOutput) .reprompt(speakOutput) .getResponse(); &#125;&#125;;const CancelAndStopIntentHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest' &amp;&amp; (Alexa.getIntentName(handlerInput.requestEnvelope) === 'AMAZON.CancelIntent' || Alexa.getIntentName(handlerInput.requestEnvelope) === 'AMAZON.StopIntent'); &#125;, handle(handlerInput) &#123; const speakOutput = 'Goodbye!'; return handlerInput.responseBuilder .speak(speakOutput) .getResponse(); &#125;&#125;;const SessionEndedRequestHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'SessionEndedRequest'; &#125;, handle(handlerInput) &#123; // Any cleanup logic goes here. return handlerInput.responseBuilder.getResponse(); &#125;&#125;;// The intent reflector is used for interaction model testing and debugging.// It will simply repeat the intent the user said. You can create custom handlers// for your intents by defining them above, then also adding them to the request// handler chain below.const IntentReflectorHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest'; &#125;, handle(handlerInput) &#123; const intentName = Alexa.getIntentName(handlerInput.requestEnvelope); const speakOutput = `You just triggered $&#123;intentName&#125;`; return handlerInput.responseBuilder .speak(speakOutput) //.reprompt('add a reprompt if you want to keep the session open for the user to respond') .getResponse(); &#125;&#125;;// Generic error handling to capture any syntax or routing errors. If you receive an error// stating the request handler chain is not found, you have not implemented a handler for// the intent being invoked or included it in the skill builder below.const ErrorHandler = &#123; canHandle() &#123; return true; &#125;, handle(handlerInput, error) &#123; console.log(`~~~~ Error handled: $&#123;error.stack&#125;`); const speakOutput = `Sorry, I had trouble doing what you asked. Please try again.`; return handlerInput.responseBuilder .speak(speakOutput) .reprompt(speakOutput) .getResponse(); &#125;&#125;;// The SkillBuilder acts as the entry point for your skill, routing all request and response// payloads to the handlers above. Make sure any new handlers or interceptors you've// defined are included below. The order matters - they're processed top to bottom.exports.handler = Alexa.SkillBuilders.custom() .addRequestHandlers( LaunchRequestHandler, HelloWorldIntentHandler, HelpIntentHandler, CancelAndStopIntentHandler, SessionEndedRequestHandler, IntentReflectorHandler, // make sure IntentReflectorHandler is last so it doesn't override your custom intent handlers ) .addErrorHandlers( ErrorHandler, ) .lambda(); 我們剛建立了一個 Intent，也需要在這段程式碼中加入 Handler 讓 Intent 被出發時做出相對應的事情：//...const DoorIntentHandler = &#123; canHandle(handlerInput) &#123; return Alexa.getRequestType(handlerInput.requestEnvelope) === 'IntentRequest' &amp;&amp; Alexa.getIntentName(handlerInput.requestEnvelope) === 'Door'; &#125;, handle(handlerInput) &#123; const speakOutput = 'Door Open!'; return handlerInput.responseBuilder .speak(speakOutput) .getResponse(); &#125;&#125;;//...exports.handler = Alexa.SkillBuilders.custom() .addRequestHandlers( LaunchRequestHandler, HelloWorldIntentHandler, DoorIntentHandler, // 加入這行剛剛加上的 Handler HelpIntentHandler, CancelAndStopIntentHandler, SessionEndedRequestHandler, IntentReflectorHandler, ) .addErrorHandlers( ErrorHandler, ) .lambda(); 完成後，點擊上方 [Save] 並且 [Deploy]。 測試 Alexa Skill部署完成後，點選上方導航列的 Test，並且將 [Skill testing is enabled in] 切換成 Development。接著，我們就可以在下方輸入框輸入想說的話來測試 Skill。 可以看到有如預期的回覆，這樣一個簡單的客製化 Alexa Skill 大功告成！","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"AWS","slug":"技術/AWS","permalink":"http://ellis-wu.github.io/categories/技術/AWS/"}],"tags":[{"name":"Amazon","slug":"Amazon","permalink":"http://ellis-wu.github.io/tags/Amazon/"},{"name":"Alexa","slug":"Alexa","permalink":"http://ellis-wu.github.io/tags/Alexa/"}]},{"title":"更換 AVS Device SDK 的喚醒詞","slug":"avs-device-sdk-wwe","date":"2019-07-16T16:00:00.000Z","updated":"2019-08-15T06:12:33.933Z","comments":true,"path":"2019/07/17/avs-device-sdk-wwe/","link":"","permalink":"http://ellis-wu.github.io/2019/07/17/avs-device-sdk-wwe/","excerpt":"在 AVS Device SDK 中有提供 Wake Word Engine (WWE) 方便讓開發者可以換成自己的喚醒詞，而官方提供的範例預設使用了Sensory所訓練出來的 Alexa 模型。但Sensory官方文件說明它是有 120 天的使用限制的，若要當作產品使用則需要與他們聯絡，且Sensory目前也僅提供 Alexa 這個喚醒詞，但 Alexa 這個詞因為發音不正確的關係實在有夠難喚醒的。因此，本文將Sensory換成Snowboy並且更改由Snowboy提供的其他喚醒詞。","text":"在 AVS Device SDK 中有提供 Wake Word Engine (WWE) 方便讓開發者可以換成自己的喚醒詞，而官方提供的範例預設使用了Sensory所訓練出來的 Alexa 模型。但Sensory官方文件說明它是有 120 天的使用限制的，若要當作產品使用則需要與他們聯絡，且Sensory目前也僅提供 Alexa 這個喚醒詞，但 Alexa 這個詞因為發音不正確的關係實在有夠難喚醒的。因此，本文將Sensory換成Snowboy並且更改由Snowboy提供的其他喚醒詞。 事前準備這邊需要先執行過 AVS Device SDK 的範例才能照本文教學執行，否則會有些檔案找不到。若沒有執行過可以參考在 Raspberry Pi 上使用 AVS Device SDK 實現智慧音箱這篇文章。 修改檔案因為Snowboy目前的教學文是透過 git patch 方式去替換一些檔案，但 AVS Device SDK 有更改檔案的路徑，因此該 patch 是無法正常使用，所以本文將一步步以手動的方式來說明該修改哪些檔案。 修改 pi.sh若執行過 AVS Device SDK 的範例，則pi.sh會存放在/home/pi/pi.sh。 首先，修改CMAKE相關資訊：# 原本為：CMAKE_PLATFORM_SPECIFIC=(-DSENSORY_KEY_WORD_DETECTOR=ON \\ -DGSTREAMER_MEDIA_PLAYER=ON -DPORTAUDIO=ON \\ -DPORTAUDIO_LIB_PATH=\"$THIRD_PARTY_PATH/portaudio/lib/.libs/libportaudio.$LIB_SUFFIX\" \\ -DPORTAUDIO_INCLUDE_DIR=\"$THIRD_PARTY_PATH/portaudio/include\" \\ -DSENSORY_KEY_WORD_DETECTOR_LIB_PATH=$THIRD_PARTY_PATH/alexa-rpi/lib/libsnsr.a \\ -DSENSORY_KEY_WORD_DETECTOR_INCLUDE_DIR=$THIRD_PARTY_PATH/alexa-rpi/include)# 修改成：CMAKE_PLATFORM_SPECIFIC=(-DKITTAI_KEY_WORD_DETECTOR=ON \\ -DGSTREAMER_MEDIA_PLAYER=ON -DPORTAUDIO=ON \\ -DPORTAUDIO_LIB_PATH=\"$THIRD_PARTY_PATH/portaudio/lib/.libs/libportaudio.$LIB_SUFFIX\" \\ -DPORTAUDIO_INCLUDE_DIR=\"$THIRD_PARTY_PATH/portaudio/include\" \\ -DKITTAI_KEY_WORD_DETECTOR_LIB_PATH=$THIRD_PARTY_PATH/snowboy/lib/rpi/libsnowboy-detect.a \\ -DKITTAI_KEY_WORD_DETECTOR_INCLUDE_DIR=$THIRD_PARTY_PATH/snowboy/include \\ -DCMAKE_BUILD_TYPE=MINSIZEREL) 修改相依性套件：# 原本為：sudo apt-get -y install git gcc cmake build-essential libsqlite3-dev libcurl4-openssl-dev libssl1.0-dev libfaad-dev libsoup2.4-dev libgcrypt20-dev libgstreamer-plugins-bad1.0-dev gstreamer1.0-plugins-good libasound2-dev sox gedit vim python3-pip# 修改成：sudo apt-get -y install git gcc cmake build-essential libsqlite3-dev libcurl4-openssl-dev libssl1.0-dev libfaad-dev libsoup2.4-dev libgcrypt20-dev libgstreamer-plugins-bad1.0-dev gstreamer1.0-plugins-good libasound2-dev sox gedit vim python3-pip libatlas-base-dev (Options) 修改腳本輸出：# 原本為：echo \"==============&gt; CLONING AND BUILDING SENSORY ==============\"# 修改成：echo \"==============&gt; CLONING AND BUILDING KITTAI ==============\" 將原本從Sensory下載相關檔案改為Snowboy：# 原本為：git clone git://github.com/Sensory/alexa-rpi.gitbash ./alexa-rpi/bin/license.sh# 修改成：git clone https://github.com/Kitt-AI/snowboy.gitcp snowboy/resources/alexa/alexa-avs-sample-app/alexa.umdl snowboy/resources/alexa.umdl 這邊可以從 Snowboy 的 GitHub 找到它提供的其他喚醒詞模型，但是不論使用哪個喚醒詞，都要把它命名為alexa.umdl。因為 AVS Device SDK 的範例程式只認得這個檔案名稱。 修改執行 AVS Device SDK 範例指令：# 原本為：./SampleApp \"$OUTPUT_CONFIG_FILE\" \"$THIRD_PARTY_PATH/alexa-rpi/models\" DEBUG9# 修改成：./SampleApp \"$OUTPUT_CONFIG_FILE\" \"$THIRD_PARTY_PATH/snowboy/resources\" DEBUG9 修改完成後即可儲存。 修改 setup.sh若執行過 AVS Device SDK 的範例，則setup.sh會存放在/home/pi/setup.sh。 首先，新增一個變數：LIB_SUFFIX=\"a\"ANDROID_CONFIG_FILE=\"\"# 新增下面這行BUILDTYPE=\"MINSIZEREL\" 接著，找到以下資訊並新增一行指令：cmake \"$SOURCE_PATH/avs-device-sdk\" \\ -DCMAKE_BUILD_TYPE=DEBUG \\ \"$&#123;CMAKE_PLATFORM_SPECIFIC[@]&#125;\"# 新增下面這行sed -E -i \"s:CXX_PLATFORM_DEPENDENT_FLAGS_\"$BUILDTYPE\"\\s+\\\"(.*)\\\":CXX_PLATFORM_DEPENDENT_FLAGS_\"$BUILDTYPE\" \\\"\\1 -D_GLIBCXX_USE_CXX11_ABI=0 -pg\\\":\" ../avs-device-sdk/build/cmake/BuildOptions.cmakecd $BUILD_PATHmake SampleApp -j2 修改完成後即可儲存。 重新編譯 AVS Device SDK 與範例重新編譯 AVS Device SDK 與範例程式之前，我們需要刪除一些檔案。因為，現在setup.sh會檢查一些檔案路徑來判斷是否需要重新安裝套件或編譯檔案。若不刪除，剛剛的改動有部分指令會沒有執行到：$ cd /home/pi/$ sudo rm -rf build/ avs-device-sdk/ 刪除後，透過以下指令開始重新安裝套件與編譯 AVS Device SDK：sudo bash setup.sh config.json -s 998987 因為是重新編譯 AVS Device SDK，所以整個安裝過程大約 15 至 20 分鐘。 更換喚醒詞安裝完成後，預設的喚醒詞依然是 Alexa，所以我們要使用Snowboy提供的其他喚醒詞來替換掉原本的 “Alexa”。$ sudo cp /home/pi/third-party/snowboy/resources/models/jarvis.umdl /home/pi/third-party/snowboy/resources/alexa.umdl 這邊選擇 computer 這個喚醒詞，可以依照自己情況選擇其他的喚醒詞。 開始體驗選擇並且替換喚醒詞後，可以按照原本使用 AVS Device SDK 的範例一樣使用：$ sudo bash startsample.sh 看到以下資訊後，就可以用你替換的喚醒詞叫醒它了!######################################### Alexa is currently idle! #########################################","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"AWS","slug":"技術/AWS","permalink":"http://ellis-wu.github.io/categories/技術/AWS/"}],"tags":[{"name":"Amazon","slug":"Amazon","permalink":"http://ellis-wu.github.io/tags/Amazon/"},{"name":"Alexa","slug":"Alexa","permalink":"http://ellis-wu.github.io/tags/Alexa/"},{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"http://ellis-wu.github.io/tags/Raspberry-Pi/"}]},{"title":"在 Raspberry Pi 上使用 AVS Device SDK 實現智慧音箱","slug":"avs-device-sdk-installation","date":"2019-07-14T16:00:00.000Z","updated":"2019-08-14T02:29:12.164Z","comments":true,"path":"2019/07/15/avs-device-sdk-installation/","link":"","permalink":"http://ellis-wu.github.io/2019/07/15/avs-device-sdk-installation/","excerpt":"Amazon 提供了 Alexa Voice Service(AVS) 的服務，希望能讓開發者可以整合 Alexa 至他們的產品之中，並且讓任何連接的設備帶來語音的便利性。Amazon 提供了 AVS Device SDK 讓開發人員可以快速且簡單的建立自己的 Alexa 產品，而 AVS Device SDK 是一個基於 C++ 的 Library，其中包含了語音的捕捉、聲音的處理與通訊等功能，而每個元件都提供 API 給開發者使用與客製化整合至自己的產品中。","text":"Amazon 提供了 Alexa Voice Service(AVS) 的服務，希望能讓開發者可以整合 Alexa 至他們的產品之中，並且讓任何連接的設備帶來語音的便利性。Amazon 提供了 AVS Device SDK 讓開發人員可以快速且簡單的建立自己的 Alexa 產品，而 AVS Device SDK 是一個基於 C++ 的 Library，其中包含了語音的捕捉、聲音的處理與通訊等功能，而每個元件都提供 API 給開發者使用與客製化整合至自己的產品中。 AVS Device SDK 架構如下圖所示，說明 AVS Device SDK 各個元件之間的資料流。 Audio Signal Processor (ASP)它是一個第三方軟體，主要為處理聲音訊號輸入與輸出使其獲得一個乾淨的聲音訊號，其中可能包含回音消除 (Acoustic Echo Cancellation, AEC)、固定或者自適應的波束成型 (Beamforming)、語音活性檢測 (Voice activity detection, VAD) 以及動態範圍壓縮 (Dynamic Range Compression, DRC)。若使用麥克風陣列，則 ASP 建構並輸出陣列的單個音頻流。 Shared Data Stream (SDS)它是一個單一的 producer 或 multi-consumer buffer，其允許在一個寫入者 (writer) 或多個讀取者 (reader) 之間傳送任何類型的資料。而 SDS 有兩個主要的功能： 發送到 AVS 之前，在音頻前端 (or Audio Signal Processor)、Wake Word Engine (WWE) 以及 Alexa Communications Library (ACL) 之間傳遞音訊資料。 透過 Alaexa Communications Library (ACL) 將由 AVS 發送的資料附件傳遞給特定的 Capbility Agents。 SDS 在 product-specific 記憶體分段 (or user-specif) 上的環形緩衝區 (ring buffer) 所實現的，這允許將其用於行程或行程間通訊 (interprocess communication)。請注意，寫入者 (writer) 和讀取者 (reader) 可能是不同的執行緒或行程。 Wake Word Engine (WWE)它是一個在輸入串流中發現識別詞的軟體，由兩個 binary interfaces 組成。第一個處理喚醒詞識別 (或檢測)；第二個處理特定的喚醒詞模型 (在官方 AVS Device SDK 中的範例是 Alexa)。而這取決於你如何實現，WWE 可以在 SOC 或專用的晶片上執行，像是數位信號處理器 (Digital Singal Processor, DSP)。 Audio Input Processor (AIP)處理透過 Alexa Communications Library (ACL) 發送到 AVS 的音頻輸入，這些包括設備上的麥克風、遠程麥克風和其他音訊輸入源。而 Audio Input Processor (AIP) 還包括了再不同音頻輸入源之間切換的邏輯，在給定的時間內只能將一個音頻輸入源發送到 AVS。 Alexa Communications Library (ACL)是客戶端與 AVS 之間的溝通的主要橋樑。\b並執行兩個關鍵功能： 與 AVS 建立並維護長時間的持久連線。ACL 遵守訊息規範的詳細說明，請參考 Manage HTTP/2 Request to AVS。 提供訊息的傳送與接收的能力，其中包括支援 JSON 格式與二進制音頻內容。請參考 Structuring an HTTP/2 Request to AVS。 Alexa Directive Sequencer Library (ADSL)管理從 AVS 來的指令順序與命令，請參考 AVS Interaction Model。該元件管理每個指令的生命週期並通知 Directive Handler (可能是也可能不是 Capability Agents) 來處理訊息。 Activity Focus Manager Library (AFML)為設備提供 audiovisual focus 的集中式管理，Focus 是基於 channels 的，如 AVS Interaction Model 中所描述，用於管理 audiovisual focus 輸入與輸出的優先順序。 Channels 可以在前景或背景，而在同一個時間，只有一個 channel 可以在前景並且擁有 focus。如果多個 channels 處於 active 狀態，則需要遵循以下優先順序：Dialog &gt; Alert &gt; Content。當在前景的 channel 變為 inactive 狀態，則優先順序中的下一個 active channel 將移至前景。 Focus 管理並不限定 Capability Agents 或 Directive Handlers，也可以由非 Alexa 相關 agents 使用。它允許所有使用 Activity Focus Manager Library (AFML) 的所有 agent 在設備上具有一致的 Focus。 Capability Agents處理 Alexa-driven 的交互；特別是指令和事件。每個 capability agent 對應於 AVS API 公開的特定 interfaces，這些 interfaces 包括： Alerts：設定、停止和刪除計時器與警報的 interface。 AudioPlayer：管理與控制 audio playback 的 interface。 Bluetooth：管理裝置與 Alexa-enabled product 之間藍芽連線的 interface。 DoNotDisturb：啟用勿擾功能的 interface。 EqualizerController：調整等化器設定的 interface；例如：分貝等級和模式。 InteractionModel：此 interface 允許客戶端支援 Alexa 發起的複雜交互；例如：Alexa Routines。 Notifications：顯示通知指示器的 interface。 PlaybackController：透過 GUI 或按鈕導向至播放佇列的 interface。 Speaker：音量控制且包含靜音與取消靜音的 interface。 SpeechRecognizer：語音捕獲的 interface。 SpeechSynthesizer：Alexa 語音輸出的 interface。 System：將產品狀態與狀況傳送給 AVS 的 interface。 TemplateRuntime：呈現可視化 metadata 的 interface。 安裝 AVS Device SDKAVS Device SDK 提供一些腳本來快速安裝與與建構並且啟用了喚醒詞。喚醒詞部分目前有兩個較知名的開源版\b (Sensory 與 Snowboy)，而 AVS Device SDK 官方提供的的快速安裝教學中的 Wake Word Engine (WWE) 預設是使用Sensory，但Snowboy亦是個不錯的選擇，它提供了額外幾種訓練好的喚醒詞；像是Computer與Jarvis等等。 事前準備本文教學是在 Raspberry Pi 上安裝 AVS Device SDK。所以執行 AVS 前需要準備一些東西： 麥克風，需要將 Raspberry Pi 上的 Audio 輸出設定此麥克風。 喇叭，可以用外接喇叭或者用耳機接至 Raspberry Pi 上的 3.5 mm 音源孔。 Rasberry Pi 必須擁有網路連線能力。 Raspbian Stretch With Desktop。 本篇嘗試使用 AVS 時，Raspbian 已經釋出 Buster 版本，但 Buster 版本在安裝過程中會有些套件版本上有問題。若裝 Jessie 版本則有些套件需要自己 build，可以參考這篇安裝說明。 若要下載 Raspbian Stretch 版本或各個版本，可以從這裡尋找。 準備安裝腳本當以上東西都準備完成後，需要先下載幾個安裝腳本：$ sudo apt-get update$ wget https://raw.githubusercontent.com/alexa/avs-device-sdk/master/tools/Install/setup.sh \\ wget https://raw.githubusercontent.com/alexa/avs-device-sdk/master/tools/Install/genConfig.sh \\ wget https://raw.githubusercontent.com/alexa/avs-device-sdk/master/tools/Install/pi.sh 授權 (Authorization)在使用 AVS Device SDK 所提供的 SampleApp 之前，需要到 Alexa Cloud 上設定一些相關資訊，讓 Raspberry Pi 能進行授權使其能使用 Alexa 的功能。 註冊產品與建立 Security Profile當註冊 Amazon developer account 後，皆要建立一個新的 Alexa 產品與 Security profile。步驟如下： 登入 Amazon Developer Portal。 上方導航列選擇 [Alexa] 後，下拉式選單中選擇 [Alexa Voice Service]。 點擊 [Create Product]。 接下來，開始創建一個 Alexa Product，所以要提供並填入一些資訊： Product Name：這是會呈現給使用者看的；當使用者\b註冊該產品時會顯示的名稱。 Product ID：該產品的 identifier。 Product Type 選擇 [Device with Alexa built-in]。 選擇後，會詢問是否將會使用 companion app，這邊選擇 [No]。 Product Category 選擇 [Other] 並且輸入 [Sample build on Raspberry Pi]。(若是商業產品，請依照自己的情況選擇。) 輸入 Brief product description。 使用者如何與產品交互，請依照自己的情況選擇。範例如下： (Options) 目前可以跳過上傳圖片的步驟。而該圖片是產品的圖示，在 amazon.com 上可以搜尋到該產品且顯示該圖片。 是否將會發行該產品，這邊選擇 [No]。(若是商業產品，請選擇 [Yes]。) 是否有使用到 Alexa Business 與 AWS IOT Core，這請依照自己的情況選擇。 這是否是兒童產品或針對 13 歲以下孩童的產品，請選擇 [No]。 以上資訊都填寫完成後，可以按下 [NEXT] 按鈕。若都沒有問題，會轉跳至 Login in With (LWA) Security Profile 的頁面。這是將使用者資料和安全憑證 (security credentials) 與產品關聯的動作。因此，這邊建立一個新的 Security Profile： 選擇 [Create New Profile]。 輸入 Security Profile 的名稱與描述，輸入完成後點擊 [Next]。 再來會產生出Client ID等資訊。這邊我們在 Platform information 的導航列上選擇 [Other devices and platforms]，並且輸入 [Client ID] 後，按下 [Generate ID] 按鈕。 接著會看到 [Download] 按鈕點擊並下載它為config.json，這檔案包含了與 security profile 相關的clientID與productID。而這個檔案將會在後續執行 AVS Device SDK 提供的 SampleApp 用到。 閱讀並同意 Amazon Developer Services Agreement, including the Alexa Voice Services Program Requirements。接著按下 [Finish] 按鈕。 若填寫資訊沒問題會彈出 Your Product has been created 的視窗。 接下來，我們要啟用剛剛的 Security Profile： 打開瀏覽器並前往 LWA Console。 在 [Select a Security Profile] 的下拉式選單選擇剛剛建立的 Security Profile，並點擊 [Confirm] 按鈕。 輸入 [privacy policy URL]。 (Options) 上傳圖片，而該圖片會在使用者進行 LWA 時會顯示的圖片。 若以上資訊都填寫完成後，按下 [Save] 按鈕。 儲存成功後，可以看到剛剛為 Alexa Product 建立的 Security Profile 已經啟用。 執行安裝腳本將剛剛在 Security Profile 中下載的config.json放至/home/pi。透過腳本進行安裝：$ cd /home/pi$ sudo bash setup.sh config.json -s 998987 -s為這台裝置的 serial number，可以把它視為這台裝置的唯一碼。 若不輸入這個參數預設為123456。 當上面指令輸入後，馬上會跳出 AVS Device SDK 的服務條款，會要你輸入 agree 同意使用第三方的 library，再按下 Enter 繼續安裝。################################################################################################################################################################AVS Device SDK Raspberry pi Script - Terms and AgreementsThe AVS Device SDK is dependent on several third-party libraries, environments,and/or other software packages that are installed using this script fromthird-party sources (\"External Dependencies\"). These are terms and conditionsassociated with the External Dependencies(available at https://github.com/alexa/avs-device-sdk/wiki/Dependencies) thatyou need to agree to abide by if you choose to install the External Dependencies.If you do not agree with every term and condition associated with the ExternalDependencies, enter \"QUIT\" in the command line when prompted by the installer.Else enter \"AGREE\".################################################################################################################################################################ 安裝過程中，會停止並且等待接受 Sensory Wake Word 的服務條款，這時按下 Enter 後在輸入 yes 即可。==============&gt; CLONING AND BUILDING SENSORY ==============Cloning into 'alexa-rpi'...remote: Enumerating objects: 232, done.remote: Counting objects: 100% (232/232), done.remote: Compressing objects: 100% (149/149), done.remote: Total 232 (delta 73), reused 232 (delta 73), pack-reused 0Receiving objects: 100% (232/232), 9.32 MiB | 1.95 MiB/s, done.Resolving deltas: 100% (73/73), done.The Sensory TrulyHandsfree binaries are not licensed.Press RETURN to review the license agreement and update the files. 整個安裝過程大約 20 分鐘。 執行範例程式安裝完成後，透過以下指令執行 AVS Device SDK 的範例程式：$ cd /home/pi$ sudo bash startsample.sh 若之後要重新執行都是使用上面的指令。 會看到以下畫面等待進行授權啟用：####################################################### &gt; &gt; &gt; &gt; &gt; NOT YET AUTHORIZED &lt; &lt; &lt; &lt; &lt; #################################################################################################################################################### To authorize, browse to: 'https://amazon.com/us/code' and enter the code: &#123;XXXX&#125; ############################################################################################# 此時，複製上面的網址並在瀏覽器打開後，輸入 Amazon 帳號密碼進行登入，在輸入 code 來完成授權與驗證。 等待驗證成功後 (可能需要等待 30 秒左右)，成功後會看到類似以下資訊：######################################### Alexa is currently idle! ######################################### 若看到以上訊息，AVS Device SDK 的 SampleApp 已經正常執行。而下次啟動就不需要在授權與驗證會自動授權與驗證。 過程中會看到許多 log 資訊。因此這是範例程式為了方便 debug，所以才會顯示很多 log 資訊。 Talk with Alexa此時，已經在 Raspberry Pi 上執行 Alexa 了，可以透過麥克風來體驗 Alexa 帶來的便利。","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"AWS","slug":"技術/AWS","permalink":"http://ellis-wu.github.io/categories/技術/AWS/"}],"tags":[{"name":"Amazon","slug":"Amazon","permalink":"http://ellis-wu.github.io/tags/Amazon/"},{"name":"Alexa","slug":"Alexa","permalink":"http://ellis-wu.github.io/tags/Alexa/"},{"name":"Raspberry Pi","slug":"Raspberry-Pi","permalink":"http://ellis-wu.github.io/tags/Raspberry-Pi/"}]},{"title":"使用 Kustomize 管理 Kubernetes 配置檔","slug":"kustomize-introduction","date":"2018-07-25T16:00:00.000Z","updated":"2019-07-22T02:21:17.798Z","comments":true,"path":"2018/07/26/kustomize-introduction/","link":"","permalink":"http://ellis-wu.github.io/2018/07/26/kustomize-introduction/","excerpt":"Kustomize 是 SIG-CLI 的一個子項目，其目的是為 Kubernetes 提供一種可以重複使用配置檔的管理工具。舉例來說，我們今天開發了一個前端應用 v1 並且撰寫了一個 Deployment、Service 以及 ConfigMap 的 YAML，接著透過kubectl指令將其部署在 Kubernetes 環境之中。但之後此前端應用經過開發與升級至 v2 並需要將其部署至 Kubernetes 環境，但其中 Kubernetes 的配置檔有些參數與 v1 略有不同。此時，我們通常會將 v1 所撰寫的 YAML 複製一份並修改部分內容，再透過kubectl部署到 Kubernetes 環境之中。但這樣的情況下我們同時保存兩份 YAML 且其他人也無法輕易地看出兩份 YAML 之間的配置有哪些不同。而 Kustomize 就可以很好的幫我們解決這些問題。","text":"Kustomize 是 SIG-CLI 的一個子項目，其目的是為 Kubernetes 提供一種可以重複使用配置檔的管理工具。舉例來說，我們今天開發了一個前端應用 v1 並且撰寫了一個 Deployment、Service 以及 ConfigMap 的 YAML，接著透過kubectl指令將其部署在 Kubernetes 環境之中。但之後此前端應用經過開發與升級至 v2 並需要將其部署至 Kubernetes 環境，但其中 Kubernetes 的配置檔有些參數與 v1 略有不同。此時，我們通常會將 v1 所撰寫的 YAML 複製一份並修改部分內容，再透過kubectl部署到 Kubernetes 環境之中。但這樣的情況下我們同時保存兩份 YAML 且其他人也無法輕易地看出兩份 YAML 之間的配置有哪些不同。而 Kustomize 就可以很好的幫我們解決這些問題。 因此，本文將使用 Kustomize 官方提供的範例，帶著大家快速了解 Kustomize 的好處與便利性。 Kustomize 安裝首先，Kustomize 的安裝十分簡單，我們這邊使用官方提供的 binary 檔案進行安裝。而根據作業系統的不同 Kustomize 提供三種 binary 檔 (linux、darwin 以及 windows)，請使用者依照自己的作業系統自行將下面的opsys參數更換： $ OP_SYSTEM=linux$ curl -s https://api.github.com/repos/kubernetes-sigs/kustomize/releases/latest | \\ grep browser_download | \\ grep $&#123;OP_SYSTEM&#125; | \\ cut -d '\"' -f 4 | \\ xargs curl -O -L$ mv kustomize_*_$&#123;OP_SYSTEM&#125;_amd64 /usr/local/bin/kustomize$ chmod u+x /usr/local/bin/kustomize Kustomize 該如何使用Kustomize 在建立一個 Kubernetes 應用程式時，它的資料夾架構類似下面這樣：~/someApp├── base│ ├── deployment.yaml│ ├── kustomization.yaml│ └── service.yaml└── overlays ├── development │ ├── cpu_count.yaml │ ├── kustomization.yaml │ └── replica_count.yaml └── production ├── cpu_count.yaml ├── kustomization.yaml └── replica_count.yaml Kustomize 需要先針對該應用建立一個基礎的 Kubernetes 配置檔，這裡稱之為 Base。之後再以 Overlay 來區分每個不同版本的配置。而 Overlay 中的 deployment 與 production 僅需要撰寫與 Base 之間有差異的設定 (例如：Image 版本不同、Replica 數量不同或資源分配不同…等差異) 以及kustomize.yaml描述該版本與 Base 之間的差異。之後透過 Kustomize 就可以分別 print 出 development 與 production 的 Kubernetes 配置檔。 國際範例 Helloworld當 Kustomize 安裝好了後，我們可以執行一個 Helloworld 範例。此範例由 Kustomize 官方提供且是一個由 Go 語言所撰寫的 web server。這邊希望藉由此範例能帶領大家能快速體驗 Kustomize 的好處。 此 Helloworld 範例透過讀取環境變數ALT_GREETING的資訊來呈現在 web server 上，並在啟動 web server 時透過enableRiskyFeature參數來改變 web server 上呈現的字體是否為斜體。 若想了解 HelloWorld 範例的程式碼，請參考 這裡。 建立 Base當我們稍微了解 Kustomize 與 HelloWorld 範例後，我們需要先建立 Base 資料夾並準備 HelloWorld 範例所需的 YAML 檔。這邊透過 Git 取得範例的 YAML 檔：$ mkdir -p kustomize/helloworld-example &amp;&amp; cd kustomize/$ DEMO_HOME=$(pwd)/helloworld-example$ BASE=$DEMO_HOME/base$ mkdir -p $BASE$ curl -s -o \"$BASE/#1.yaml\" \\ \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/examples/helloWorld/&#123;configMap,deployment,kustomization,service&#125;.yaml\"$ tree $DEMO_HOME/root/kustomize/helloworld-example└── base ├── configMap.yaml ├── deployment.yaml ├── kustomization.yaml └── service.yaml1 directories, 4 files 接下來，我們可以使用kustomize build 指令來產生出 Base 的 Kubernetes 配置檔：$ kustomize build $BASE 可以使用以下方式來將 Base 部署於 Kubernetes 之中：$ kustomize build $BASE | kubectl apply -f -configmap \"the-map\" createdservice \"the-service\" createddeployment.apps \"the-deployment\" created$ kubectl get po,svc,configmapNAME READY STATUS RESTARTS AGEpod/the-deployment-f9f46f89f-j5hcs 1/1 Running 0 17spod/the-deployment-f9f46f89f-q7c7w 1/1 Running 0 17spod/the-deployment-f9f46f89f-znjrn 1/1 Running 0 17sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 17dservice/the-service LoadBalancer 10.105.224.225 &lt;pending&gt; 8666:30325/TCP 17sNAME DATA AGEconfigmap/the-map 2 17s 可以透過瀏覽器查看 Helloworld 範例的 web server 或者使用curl來確認： $ curl 172.20.3.57:30325&lt;html&gt;&lt;body&gt;Version 1 : Good Morning!&lt;/body&gt;&lt;/html&gt; 可以看到ConfigMap中定義了兩個參數並在Deployment中來讀取這兩個參數。所以在 web server 上會呈現 Good Morning 字樣且字體為正常的字體。而kustomize.yaml中宣告此應用程式的commonLabels為app: hello。因此，kustomize 會將每個 Kubernetes 配置檔中加入一個app: hello的 label。再來定義此應用程式是由為deployment.yaml、service.yaml與configMap.yaml三個檔案所組成。 建立 Overlay上面測試完成後，我們可以開始建立 Overlay 的資料夾結構，這邊為了看出 kustomize 的好處，我們將在 Overlay 底下建立staging與production兩個資料夾。Staging 將改變 configMap 中的參數；而 production 將改變 deployment 的 replicas 數量。 首先，建立 Overlay 資料夾：$ OVERLAYS=$DEMO_HOME/overlays$ mkdir -p $OVERLAYS/staging$ mkdir -p $OVERLAYS/production 接著準備 Staging 的kustomize.yaml：$ cat &lt;&lt;EOF &gt; $OVERLAYS/staging/kustomization.yamlnamePrefix: staging-commonLabels: variant: staging org: acmeCorporationcommonAnnotations: note: Hello, I am staging!bases:- ../../basepatches:- map.yamlEOF 在此範例中我們定義了幾個東西： Kubernetes 應用程式的名稱：以往我們的應用程式 v1 在跑時，想要測試 v2 版本，最簡單的方式就是複製 v1 版本的配置檔，並將其修改內容更改之後，再來去修改每個檔案中的名稱，否則重複名稱 Kubernetes 會無法建立。而 Kustomize 透過namePrefix參數為此應用程式在 Kubernetes 之中的名稱加個前綴字staging-，所以之後透過kubectl看到其名稱會變成staging-xxxxxx； 新增 label：這個配置不是必須的，但可以看到 Kustomize 可以這樣為此 Staging 的應用程式新增兩個 label； 新增 Annotations：這個配置也不是必須的，但可以看到 Kustomize 可以這樣為此 Staging 的應用程式新增 Annotations； 定義 Base：定義此應用程式的 Base； 定義 Patch：定義 Overlay 與 Base 之間需要 patches 的檔案。 接著，準備 Staging 需要 patch 的檔案：$ cat &lt;&lt;EOF &gt; $OVERLAYS/staging/map.yamlapiVersion: v1kind: ConfigMapmetadata: name: the-mapdata: altGreeting: \"Have a pineapple!\" enableRisky: \"true\"EOF 我們再來準備 Production 應用程式的設定：$ cat &lt;&lt;EOF &gt; $OVERLAYS/production/kustomization.yamlnamePrefix: production-commonLabels: variant: production org: acmeCorporationcommonAnnotations: note: Hello, I am production!bases:- ../../basepatches:- deployment.yamlEOF 我們改變 Deployment 的 replicas 數量：$ cat &lt;&lt;EOF &gt; $OVERLAYS/production/deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: the-deploymentspec: replicas: 5EOF 以上都完成後，其資料夾結構如下：tree $DEMO_HOME/root/kustomize/helloworld-example├── base│ ├── configMap.yaml│ ├── deployment.yaml│ ├── kustomization.yaml│ └── service.yaml└── overlays ├── production │ ├── deployment.yaml │ └── kustomization.yaml └── staging ├── kustomization.yaml └── map.yaml4 directories, 8 files 接下來，我們可以部署 Staging 的應用程式：$ kustomize build $OVERLAYS/staging | kubectl apply -f -configmap \"staging-the-map\" createdservice \"staging-the-service\" createddeployment.apps \"staging-the-deployment\" created$ kubectl get po,svc,configmapNAME READY STATUS RESTARTS AGEpod/staging-the-deployment-64cd4f746f-5rcqf 1/1 Running 0 3mpod/staging-the-deployment-64cd4f746f-s2b2d 1/1 Running 0 3mpod/staging-the-deployment-64cd4f746f-wqtxh 1/1 Running 0 3mpod/the-deployment-f9f46f89f-j5hcs 1/1 Running 0 8mpod/the-deployment-f9f46f89f-q7c7w 1/1 Running 0 8mpod/the-deployment-f9f46f89f-znjrn 1/1 Running 0 8mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 17dservice/staging-the-service LoadBalancer 10.102.157.42 &lt;pending&gt; 8666:30630/TCP 3mservice/the-service LoadBalancer 10.105.224.225 &lt;pending&gt; 8666:30325/TCP 8mNAME DATA AGEconfigmap/staging-the-map 2 3mconfigmap/the-map 2 8m 確認部署完畢後，我們一樣可以透過瀏覽器查看 Staging 的 web server 或者使用curl來確認 web server 上的資訊以及字體的改變： $ curl 172.20.3.57:30630&lt;html&gt;&lt;body&gt;Version 1 : &lt;em&gt;Have a pineapple!&lt;/em&gt;&lt;/body&gt;&lt;/html&gt; 接下來，我們再來部署 Production 來看看結果：$ kustomize build $OVERLAYS/production | kubectl apply -f -configmap \"production-the-map\" createdservice \"production-the-service\" createddeployment.apps \"production-the-deployment\" created$ kubectl get po,svc,configmapNAME READY STATUS RESTARTS AGEpod/production-the-deployment-54748f86c5-4ltf7 1/1 Running 0 1mpod/production-the-deployment-54748f86c5-fjcfd 1/1 Running 0 1mpod/production-the-deployment-54748f86c5-fzprs 1/1 Running 0 1mpod/production-the-deployment-54748f86c5-jqv88 1/1 Running 0 1mpod/production-the-deployment-54748f86c5-k4tp5 1/1 Running 0 1mpod/staging-the-deployment-64cd4f746f-llrb7 1/1 Running 0 8mpod/staging-the-deployment-64cd4f746f-qxt2h 1/1 Running 0 8mpod/staging-the-deployment-64cd4f746f-v687r 1/1 Running 0 8mpod/the-deployment-f9f46f89f-j5hcs 1/1 Running 0 13mpod/the-deployment-f9f46f89f-q7c7w 1/1 Running 0 13mpod/the-deployment-f9f46f89f-znjrn 1/1 Running 0 13mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 17dservice/production-the-service LoadBalancer 10.110.250.85 &lt;pending&gt; 8666:30267/TCP 1mservice/staging-the-service LoadBalancer 10.102.157.42 &lt;pending&gt; 8666:30630/TCP 8mservice/the-service LoadBalancer 10.105.224.225 &lt;pending&gt; 8666:30325/TCP 13mNAME DATA AGEconfigmap/production-the-map 2 1mconfigmap/staging-the-map 2 8mconfigmap/the-map 2 13m 因為 Production 與 Base 之間我們只改變 Deployment 的 replicas 數量，所以可以透過kubectl指令看到 Production 的 pod 數量為五個。 以上為官方提供的其中一個 Example，而其他 Example 請參考 這裡。 結論使用 Kustomize 來作為 Kubernetes 配置檔的管理看起來不錯，但對使用者來說又要額外學習如何使用 Kustomize。但其帶來的好處在於，可以快速知道 Staging 與 Production 兩個版本上做了哪些修改，管理上帶來極大的好處。而目前 Kustomize 為 SIG 的子專案，未來的發展與後續維運方面還是個未知數，但它並不依賴於 Kubernetes 所以還是可以嘗試看看。 參考資料 Kustomize Github Kuberentes Kustomize 初體驗","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Kubernetes","slug":"技術/Kubernetes","permalink":"http://ellis-wu.github.io/categories/技術/Kubernetes/"}],"tags":[{"name":"Kustomize","slug":"Kustomize","permalink":"http://ellis-wu.github.io/tags/Kustomize/"}]},{"title":"使用 Helm 在 Kubernetes 上部署 Harbor","slug":"harbor-on-kubernetes","date":"2018-07-11T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2018/07/12/harbor-on-kubernetes/","link":"","permalink":"http://ellis-wu.github.io/2018/07/12/harbor-on-kubernetes/","excerpt":"在之前已經介紹過 Harbor，且是在實體機上的透過docker-compose簡單且快速的部署 Harbor。而本篇將介紹如何透過 Kubernetes Helm 來部署 Harbor。而官方在早期有提供 YAML 來直接部署 Harbor 在 Kubernetes 之上，且 Harbor 的版本為 v1.2。但後來官方不建議直接使用 YAML 來部署 Harbor，而是透過 Helm 來部署。因此本文將介紹如何用 Kubernetes Helm 來部署 Harbor 在 Kubernetes 上。","text":"在之前已經介紹過 Harbor，且是在實體機上的透過docker-compose簡單且快速的部署 Harbor。而本篇將介紹如何透過 Kubernetes Helm 來部署 Harbor。而官方在早期有提供 YAML 來直接部署 Harbor 在 Kubernetes 之上，且 Harbor 的版本為 v1.2。但後來官方不建議直接使用 YAML 來部署 Harbor，而是透過 Helm 來部署。因此本文將介紹如何用 Kubernetes Helm 來部署 Harbor 在 Kubernetes 上。 目前 Harbor 部署在 Kubernetes v1.8+ 上，會遇到一些問題 (#5295)。本文操作皆使用 Harbor Master Branch，但 Master Branch 為開發 Branch 可能某些步驟或設定會與實際情況稍微不同。因此，目前不適合在 Production 環境中使用，可能要等到 Harbor 釋出 1.6 後再來測試看看。 事前準備目前 Harbor 官方透過 Helm 部署在 Kubernetes 上有些限制與需求，請確認以下需求： Kubernetes cluster 1.8+ with Beta APIs enabled Kubernetes Ingress Controller is enabled kubectl CLI 1.8+ Helm CLI 2.8.0+ 而本文事先準備了一個 Kubernetes 叢集，且是利用kubeadm所部署而成；環境並非實體機而為虛擬機機。再準備一台機器並安裝 NFS Server，此 NFS Server 將儲存 Harbor 一些資訊以及 Docker Images 的儲存。 IP Address Role CPU RAM Disk 172.20.3.57 Master1 2 vCPU 2 GB 40 GB 172.20.3.50 Node1 2 vCPU 2 GB 40 GB 172.20.3.55 Node2 2 vCPU 2 GB 40 GB 172.16.35.50 NFS Server 1 vCPU 2 GB 200 GB 作業系統皆為 Ubuntu 16.04； Docker 版本為 1.18.03； Kubernetes 版本為 1.10.3；Kubernetes CNI 為Calico。 利用kubeadm部署 Kubernetes 叢集，可以參考 只要用 kubeadm 小朋友都能部署 Kubernetes。 安裝 Kubernetes Nginx Ingress使用 Helm 部署 Harbor 時，會使用到 Kubernetes Ingress 將 Harbor 的各個服務 expose 出來。而本文選擇使用 Nginx Ingress 作為 Ingress Controller。以下為使用官方檔案快速部署 ingress-nginx controller：$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml 因為我們是虛擬機上部署 ingress-nginx。因此，透過NodePort將 ingress-nginx 的服務 expose 出來：$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml 其他平台要啟用 ingress-nginx 或者將 Ingress 服務 expose，請參考 這裡； Ingress 不一定要依照以上方法透過NodePort將服務 expose，可依照自己的需求替換。 完成後，透過kubectl查看：$ kubectl -n ingress-nginx get po,svcNAME READY STATUS RESTARTS AGEpod/default-http-backend-5c6d95c48-bb89j 1/1 Running 0 2mpod/nginx-ingress-controller-c6d66b4fb-7s2bh 1/1 Running 0 2mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/default-http-backend ClusterIP 10.105.57.243 &lt;none&gt; 80/TCP 2mservice/ingress-nginx NodePort 10.108.5.125 &lt;none&gt; 80:31940/TCP,443:31164/TCP 2m 這邊可以看到 ingress-nginx 透過NodePort將服務 expose 出來。若應用程式為 HTTP 協定則由 31940 port 將應用程式 expose；若為 HTTPS 協定則由 31164 port 將應用程式 expose。 安裝 Kubernetes Helm因為要使用 Helm 來部署 Harbor，所以我們需要先安裝 Helm。而 Helm 的安裝方式有很多，這邊使用 binary 的方式進行安裝：$ wget -qO- https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz | tar -zx$ mv linux-amd64/helm /usr/local/bin 接著為 Helm 設定 RBAC 並初始化 Helm：$ kubectl -n kube-system create sa tiller$ kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller$ helm init --service-account tiller 完成後，就可以透過kubectl來查看 Tiller Server 是否被建立：$ kubectl get po,svc -n kube-system -l app=helmNAME READY STATUS RESTARTS AGEpod/tiller-deploy-5c688d5f9b-s67d7 1/1 Running 0 3mNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/tiller-deploy ClusterIP 10.106.146.158 &lt;none&gt; 44134/TCP 3m 使用 Helm 部署 Harbor在使用 Helm 部署 Harbor 之前，我們需要先準備四個 Persistent Volumes 提供給 Harbor 的服務。而這邊使用 NFS 來提供四個 Persistent Volumes。 先到 NFS Server 上建立四個資料夾：$ mkdir -p /var/nfsshare/nfs&#123;1..4&#125; 接著回到 Kubernetes Master 上，透過以下指令來建立四個 Persistent Volume：$ for i in &#123;1..4&#125;; docat &lt;&lt;EOF | kubectl create -f -apiVersion: v1kind: PersistentVolumemetadata: name: pv00$&#123;i&#125;spec: capacity: storage: 50Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle nfs: path: /var/nfsshare/nfs$&#123;i&#125; server: 172.16.35.50EOFdone 以上步驟若有其他後端儲存系統，請依照自己的需求替換。 再來，使用kubectl指令確認 Persistent Volume 是否建立起來：$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpv001 50Gi RWO Recycle Available 3spv002 50Gi RWO Recycle Available 3spv003 50Gi RWO Recycle Available 2spv004 50Gi RWO Recycle Available 2s 當 Persistent Volume 準備完成後，透過 Git 取得 Harbor 官方提供的 Helm Chart ：$ git clone https://github.com/vmware/harbor.git$ cd harbor/contrib/helm/harbor/$ helm dependency update 在預設的情況下，Helm 部署的 Harbor 會是走 HTTPS 協定。因此需要修改values.yaml中的externalPort，將externalPort設定為 ingress-nginx HTTPS Port：# The Port for Harbor service, leave empty if the service# is to be bound to port 80/443externalPort: 31164 若要設定 Harbor 的其他資訊，請修改values.yaml的設定，而每個設定資訊請參考 這裡 以上設定完成後，使用 Helm 指令部署 Harbor：$ helm install . --debug --name my-harbor --set externalDomain=harbor.my.domain...NOTES:Please wait for several minutes for Harbor deployment to complete.Then you should be able to visit the UI portal at https://harbor.my.domain:31164.For more details, please visit https://github.com/vmware/harbor. 透過kubectl查看 Harbor 是否部署成功：$ kubectl get poNAME READY STATUS RESTARTS AGEmy-harbor-harbor-adminserver-0 1/1 Running 1 7mmy-harbor-harbor-clair-5f7547dc95-mwkjg 1/1 Running 1 7mmy-harbor-harbor-database-0 1/1 Running 0 7mmy-harbor-harbor-jobservice-6764bf89b6-b898c 1/1 Running 1 7mmy-harbor-harbor-notary-server-64fcd84cf5-79sfj 1/1 Running 0 7mmy-harbor-harbor-notary-signer-7d6c45fc8f-54xnf 1/1 Running 0 7mmy-harbor-harbor-registry-0 1/1 Running 0 7mmy-harbor-harbor-ui-848d95d674-rq9hv 1/1 Running 2 7mmy-harbor-postgresql-558fc8ddd6-4rl69 1/1 Running 0 7mmy-harbor-redis-master-0 1/1 Running 0 7m 若部署完成後，將harbor.my.domain加入至/etc/hosts：127.0.0.1 docker-node1 single-node1127.0.0.1 localhost127.0.1.1 vagrant.vm vagrant172.20.3.57 harbor.my.domain 接著可以透過瀏覽器查看 Harbor Web UI。 預設的管理者帳號密碼為 admin/Harbor12345 開始使用 Harbor確認 Helm 部署的 Harbor 沒問題後，可以開始使用 Harbor。以下將說明如何讓 Docker Client 如何存取私有的 Registry 以及一些基本操作。 設定 Docker 存取私有 Registry首先，要讓 Docker 能存取私有的 Registry 需要對 Docker 做一些小小的設定，而設定方式有以下兩種方式： 使用自帶的憑證(CA)：為了安全性考量，私有的 Registry 自帶憑證。當 Docker Client 與私有的 Registry 進行溝通時都需要使用此憑證，而這也是目前 Docker 官方推薦的做法； 設定 Insecure Registry：直接設定為 Insecure Registry，因為安全性的考量目前此方法官方並不推薦。 而兩種方法選擇其中一種設定即可。 設定憑證(CA)因為我們部署的 Harbor 是有自帶憑證(CA)，所以需要再 Docker Client 加入憑證，這樣 Docker Client 才有辦法存取到私有的 Registry。 首先，在 Kubernetes Master 使用以下指令取得憑證資訊：$ kubectl get secrets/my-harbor-harbor-ingress -o jsonpath=\"&#123;.data.ca\\.crt&#125;\" | base64 --decode-----BEGIN CERTIFICATE-----MIIC5TCCAc2gAwIBAgIBATANBgkqhkiG9w0BAQsFADAUMRIwEAYDVQQDEwloYXJib3ItY2EwHhcNMTgwNzE4MDIwOTQzWhcNMjgwNzE1MDIwOTQzWjAUMRIwEAYDVQQDEwloYXJib3ItY2EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg8W4quGb3TmdbBt419EG6FXtM+RZhdBiz+h0DR7/C9kU5LzjONuJCxyXn/8TDqeD6MdxfHeakWMxTclbpmmBkeu0FIErl0xA9dFBYl6hpLwcka9U9Lo0gKeVHRGqS0PXnFP6XmUSyrq24NKceBan2vx1VAqG+2m+zEGzpKjL06aXkVHtY01NbL75Xc+l9a28i46WqUHAc29XjaJDIU9pf+E2RV3IQWe9Klbf9hIOs8mtYN373bM73ma0oLRK3/4U/6SMzUHGoRSIT59LKSB9cE/qB5A7HvBJLeIMQa8se3xdfmMqEARNhz+cRtRFN2MMbbmFqy2TMANpGs+aVfMmdAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwICpDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAZnmk90vWl+uRJJO51ZQTHjmDjTA246nJ9upyXlf2S3t4TOySeXupji6O1Sj4Xf4Dcqx/4Cb+Yzz0nw7bnxnbsNUo3fIcT7FRlYPlmKyUzf6WrcgvMFSfrgTfAR0upEWDArkndN3zNESU/Kq8veJwX3AnQfabMHVU6XDQIL3jRqVlObqTVCrN14eVcJDhBu6waiaPOxduh8Jfvu0YEc3ZdmP1ZyUjFUCTjVEOl+vg3uIjWouzanZwaLutMQCtDsEH5VgLku4ir5FkCG8riyZCCqKZtbMmpJaJGJQqAiJ4+RPrdNY9eTMO4KcVrC6715h3S44aylrvleJlU2S6UHRDLg==-----END CERTIFICATE----- 取得憑證後，在每一台 Docker Client 加入以下憑證：$ mkdir /etc/docker/certs.d/harbor.my.domain:31164$ cat &lt;&lt;EOF &gt; /etc/docker/certs.d/harbor.my.domain:31164/ca.crt-----BEGIN CERTIFICATE-----MIIC5TCCAc2gAwIBAgIBATANBgkqhkiG9w0BAQsFADAUMRIwEAYDVQQDEwloYXJib3ItY2EwHhcNMTgwNzE4MDIwOTQzWhcNMjgwNzE1MDIwOTQzWjAUMRIwEAYDVQQDEwloYXJib3ItY2EwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDg8W4quGb3TmdbBt419EG6FXtM+RZhdBiz+h0DR7/C9kU5LzjONuJCxyXn/8TDqeD6MdxfHeakWMxTclbpmmBkeu0FIErl0xA9dFBYl6hpLwcka9U9Lo0gKeVHRGqS0PXnFP6XmUSyrq24NKceBan2vx1VAqG+2m+zEGzpKjL06aXkVHtY01NbL75Xc+l9a28i46WqUHAc29XjaJDIU9pf+E2RV3IQWe9Klbf9hIOs8mtYN373bM73ma0oLRK3/4U/6SMzUHGoRSIT59LKSB9cE/qB5A7HvBJLeIMQa8se3xdfmMqEARNhz+cRtRFN2MMbbmFqy2TMANpGs+aVfMmdAgMBAAGjQjBAMA4GA1UdDwEB/wQEAwICpDAdBgNVHSUEFjAUBggrBgEFBQcDAQYIKwYBBQUHAwIwDwYDVR0TAQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAZnmk90vWl+uRJJO51ZQTHjmDjTA246nJ9upyXlf2S3t4TOySeXupji6O1Sj4Xf4Dcqx/4Cb+Yzz0nw7bnxnbsNUo3fIcT7FRlYPlmKyUzf6WrcgvMFSfrgTfAR0upEWDArkndN3zNESU/Kq8veJwX3AnQfabMHVU6XDQIL3jRqVlObqTVCrN14eVcJDhBu6waiaPOxduh8Jfvu0YEc3ZdmP1ZyUjFUCTjVEOl+vg3uIjWouzanZwaLutMQCtDsEH5VgLku4ir5FkCG8riyZCCqKZtbMmpJaJGJQqAiJ4+RPrdNY9eTMO4KcVrC6715h3S44aylrvleJlU2S6UHRDLg==-----END CERTIFICATE-----EOF 設定 Insecure Registry以上使用憑證的方式是因為安全性的考量，但也可以修改 Docker 設定來使用 Insecure Registry。但這樣並不安全所以不建議使用。 新增--insecure-registry參數至/etc/default/docker中：DOCKER_OPTS=\"--insecure-registry harbor.my.domain:31164\" 若在 Ubuntu 16.04 版本，還需要修改/lib/systemd/system/docker.service檔案：EnvironmentFile=-/etc/default/%pExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS 其他細節，請參考 Test an insecure registry 修改完成後，重新啟動服務：$ sudo systemctl daemon-reload$ sudo systemctl restart docker.service 登入 Docker client以上設定完成後，可以透過 Docker 指令進行 login：$ docker login harbor.my.domain:31164 Push Image將映像檔上 tag 之後，上傳至 Harbor：$ docker tag ubuntu:16.04 harbor.my.domain:31164/&lt;your project&gt;/ubuntu:16.04$ docker push harbor.my.domain:31164/&lt;your project&gt;/ubunut:16.04 可以在 Harbor Web UI 上看到剛剛 push 的 docker image。 Pull Image$ docker pull &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 更多使用者操作，請參考 Harbor User Guide 參考資料 Harbor Github Kuberentes Helm 介紹 ingress-nginx Github","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Kubernetes","slug":"技術/Kubernetes","permalink":"http://ellis-wu.github.io/categories/技術/Kubernetes/"}],"tags":[{"name":"Harbor","slug":"Harbor","permalink":"http://ellis-wu.github.io/tags/Harbor/"},{"name":"Docker Registry","slug":"Docker-Registry","permalink":"http://ellis-wu.github.io/tags/Docker-Registry/"},{"name":"Helm","slug":"Helm","permalink":"http://ellis-wu.github.io/tags/Helm/"}]},{"title":"Hexo 新增 Color Quote","slug":"hexo-color-quote","date":"2018-06-12T16:00:00.000Z","updated":"2019-08-16T03:52:48.368Z","comments":true,"path":"2018/06/13/hexo-color-quote/","link":"","permalink":"http://ellis-wu.github.io/2018/06/13/hexo-color-quote/","excerpt":"最近看到 hexo-theme-minos 提供了一個非常漂亮的 Color Quote 功能，如下面所示。因此，想把它加入到自己的部落格之中。而開始從它的 Github 中搜尋它的原始碼，發現原來它是利用 Hexo 提供的 extend tag 功能，而它的使用方法很簡單，使用者只要註冊了一個 extend tag，之後再文章中就可以使用此 extend tag，但是 extend tag 僅限於使用 Hexo 的語法；而不是原本 Markdown 的語法。","text":"最近看到 hexo-theme-minos 提供了一個非常漂亮的 Color Quote 功能，如下面所示。因此，想把它加入到自己的部落格之中。而開始從它的 Github 中搜尋它的原始碼，發現原來它是利用 Hexo 提供的 extend tag 功能，而它的使用方法很簡單，使用者只要註冊了一個 extend tag，之後再文章中就可以使用此 extend tag，但是 extend tag 僅限於使用 Hexo 的語法；而不是原本 Markdown 的語法。 因為hexo-theme-minos的 Color Quote 中的內容無法使用 Markdown 語法。因此，本文參考hexo-them-minos並進行一些修改，讓 Color Quote 中的內容可以使用 Markdown 的語法。接下來將一步步介紹如何在 Hexo 部落格加入 Color Quote。 註冊一個 Extend Tag我們要先註冊一個名為colorquote的 extend tag，這樣之後在撰寫文章時就可以使用這個 tag。 這邊要注意的一點是因為使用 Hexo 提供的 extend tag 功能，所以在撰寫文章的時候必須使用 Hexo 使用 tag 的寫法，而不是 Markdown 的語法。 首先，在themes/&lt;your theme&gt;/scripts底下建立一個名為 colorquote 的 JS 檔並輸入以下內容：/*** Color Quote Block Tag* @description Color Quote Block* @example* &lt;% colorquote [type] %&gt;* content* &lt;% endcolorquote %&gt;*/hexo.extend.tag.register('colorquote', function (args, content) &#123; var type = args[0]; var mdContent = hexo.render.renderSync(&#123;text: content, engine: 'markdown'&#125;); return '&lt;blockquote class=\"colorquote ' + type + '\"&gt;' + mdContent + '&lt;/blockquote&gt;';&#125;, &#123;ends: true&#125;); 這邊可以看到，我們註冊了一個 extend tag 名為colorquote，所以當在撰寫文章時，使用到此 extend tag 時，會將此 tag 中的 content 經過 Markdown 的渲染後，在將經過渲染後的 content 包在blockquote這個 Hexo 原有的元件之中。但此時，會為這個blockquote新增一個 class 名稱。接下來，我們只要在blockquote的 CSS 中增加樣式就可以看到效果了。 設定 Color Quote CSS當我們已經註冊好了colorquote這個 extend tag。接下來，找到自己部落格中blockquote的 css 並加入類似於以下的內容，每個人部落格的 CSS 可能都有些差異，麻煩請依照自己的需求進行調整： blockquote position: static font-family: font-serif font-size: 1.1em padding: 10px 20px 10px 54px background: rgba(0,0,0,0.03) border-left: 5px solid #ee6e73 &amp;:before top: 20px left: -40px content: \"\\f10d\" color: #e2e2e2 font-size: 32px; font-family: FontAwesome text-align: center position: relative footer font-size: font-size margin: line-height 0 font-family: font-sans cite &amp;:before content: \"—\" padding: 0 0.5em.colorquote position: relative; padding: 0.1em 1.5em; color: #4a4a4a; margin-bottom: 1em; &amp;:before content: \" \"; position: absolute; top: 50%; left: -14.5px; margin-top: -12px; width: 24px; height: 24px; border-radius: 50%; text-align: center; color: white; background-size: 16px 16px; background-position: 4px 4px; background-repeat: no-repeat; &amp;.info border-color: hsl(204, 86%, 53%); background-color: hsl(204, 86%, 93%); &amp;:before background-color: hsl(204, 86%, 53%); background-image: url(\"../images/info.svg\"); &amp;.success border-color: hsl(141, 71%, 48%); background-color: hsl(141, 70%, 88%); &amp;:before background-color: hsl(141, 71%, 48%); background-image: url(\"../images/check.svg\"); &amp;.warning border-color: hsl(48, 100%, 67%); background-color: hsl(48, 100%, 91%); &amp;:before background-color: hsl(48, 100%, 67%); background-image: url(\"../images/question.svg\"); &amp;.danger border-color: hsl(348, 100%, 61%); background-color: hsl(348, 100%, 85%); &amp;:before background-color: hsl(348, 100%, 61%); background-image: url(\"../images/exclamation.svg\"); 再來colorquote前面的圖片是一張圖片。因此，我們在themes/&lt;your theme&gt;/source/底下建立一個images資料夾，並將圖片丟進去，而圖片可以到從我的 source code 中下載：$ ls themes/icarus/source/imagescheck.svg exclamation.svg info.svg question.svg 這時候，我們可以是在文章中使用看看colorquote這個 extend tag 是否能用：&#123;% colorquote info %&#125;Example: info&#123;% endcolorquote %&#125;&#123;% colorquote success %&#125;Example: success&#123;% endcolorquote %&#125;&#123;% colorquote warning %&#125;Example: warning&#123;% endcolorquote %&#125;&#123;% colorquote danger %&#125;Example: danger&#123;% endcolorquote %&#125; Example: info Example: success Example: warning Example: danger 修改 Code Block 問題但此刻，會發現若在colorquote中的內容使用到 Markdown code block 的語法會無法正常渲染。所以為了修正此錯誤，我們在themes/&lt;your theme&gt;/scripts底下建立一個名為 tags 的 JS 檔並輸入以下內容：/*** Tags Filter* @description Fix the code block using ```&lt;code&gt;``` will render undefined in Nunjucks* https://github.com/hexojs/hexo/issues/2400*/const rEscapeContent = /&lt;escape(?:[^&gt;]*)&gt;([\\s\\S]*?)&lt;\\/escape&gt;/g;const placeholder = '\\uFFFD';const rPlaceholder = /(?:&lt;|&amp;lt;)\\!--\\uFFFD(\\d+)--(?:&gt;|&amp;gt;)/g;const cache = [];function escapeContent(str) &#123; return '&lt;!--' + placeholder + (cache.push(str) - 1) + '--&gt;';&#125;hexo.extend.filter.register('before_post_render', function(data) &#123; data.content = data.content.replace(rEscapeContent, function(match, content) &#123; return escapeContent(content); &#125;); return data;&#125;);hexo.extend.filter.register('after_post_render', function(data) &#123; data.content = data.content.replace(rPlaceholder, function() &#123; return cache[arguments[1]]; &#125;); return data;&#125;); 驗證 Color Quote基本上以上步驟都完成，就可以在文章中使用 Color Quote 且內容可以使用原本 Markdown 語法：Example: info$ echo \"This is a Color Quote Example\" Example: success$ echo \"This is a Color Quote Example\" Example: warning$ echo \"This is a Color Quote Example\" Example: danger$ echo \"This is a Color Quote Example\" 參考資料 hexo-theme-minos Hexo Tag hexo issues #2400","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Hexo","slug":"技術/Hexo","permalink":"http://ellis-wu.github.io/categories/技術/Hexo/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://ellis-wu.github.io/tags/Hexo/"}]},{"title":"企業等級的 Docker Registry Harbor","slug":"harbor-docker-registry","date":"2018-05-02T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2018/05/03/harbor-docker-registry/","link":"","permalink":"http://ellis-wu.github.io/2018/05/03/harbor-docker-registry/","excerpt":"Harbor 是一個企業級 Registry 伺服器用於儲存和分散 Docker Image 的，透過新增一些企業常用的功能，例如：安全性、身分驗證和管理等功能擴展了開源的 Docker Distribution。作為一個企業級的私有 Registry 伺服器，Harbor 提供了更好的效能與安全性。Harbor 支援安裝多個 Registry 並將 Image 在多個 Registry 做 replicated。除此之外，Harbor 亦提供了高級的安全性功能，像是用戶管理(user managment)，存取控制(access control)和活動審核(activity auditing)。","text":"Harbor 是一個企業級 Registry 伺服器用於儲存和分散 Docker Image 的，透過新增一些企業常用的功能，例如：安全性、身分驗證和管理等功能擴展了開源的 Docker Distribution。作為一個企業級的私有 Registry 伺服器，Harbor 提供了更好的效能與安全性。Harbor 支援安裝多個 Registry 並將 Image 在多個 Registry 做 replicated。除此之外，Harbor 亦提供了高級的安全性功能，像是用戶管理(user managment)，存取控制(access control)和活動審核(activity auditing)。 功能特色 基於角色為基礎的存取控制(Role based access control)：使用者和 Repository 透過 Project 進行組織管理，一個使用者在同一個 Project 下，對於每個 Image 可以有不同權限。 基於 Policy 的 Image 複製：Image 可以在多得 Registry instance 中同步複製。適合於附載平衡、高可用性、混合雲與多雲的情境。 支援 LDAP/AD：Harbor 可以整合企業已有的 LDAP/AD，來管理使用者的認證與授權。 使用者的圖形化介面：使用者可以透過瀏覽器，查詢 Image 和管理 Project 審核管理：所有對 Repositroy 的操作都被記錄。 RESTful API：RESTful APIs 提供給管理的操作，可以輕易的整合額外的系統。 快速部署：提供 Online installer 與 Offline installer。 安裝方式Harbor 提供三種方法進行安裝： Online Installer：這種安裝方式會從 Docker hub 下載 Harbor 所需的映像檔，因此 Installer 檔案較輕量； Offline Installer：當無任何網際網路連接的情況下使用此種安裝方式，預先將所需的映像檔打包，因此 Installer 檔案較大； OVA Installer：當使用者的環境是 vCenter 時使用此安裝。OVA 部署後 Harbor 才啟動，更多詳細資訊請參考 Harbor OVA install guide。 事前準備Harbor 會部署數個 Docker container，所以部署的主機需要能支援 Docker 的 Linux 發行版本。而部署主機需要安裝 Python、Docker 以及 Docker Compose 等軟體。 硬體需求： Resource Capacity Description CPU minimal 2 CPU 4 CPU is prefered RAM minimal 4 GB 8 GB is prefered Disk minimal 40 GB 160 GB is prefered 軟體需求： Software Version Python v2.7+ Docker Engine v1.10+ Docker Compose v1.6.0+ OpenSSL latest is prefered Network Port： Port Protocol Description 443 HTTPS Harbor UI 與 API 會使用此 Port 進行 HTTPS 的溝通 4443 HTTPS 若有使用 Notary 才會使用到此 Port 80 HTTP Harbor UI 與 API 會使用此 Port 進行 HTTP 的溝通 本文安裝的 Harbor 各軟體版本資訊： Harbor 版本為 v1.4.0 Docker 版本為 18.04.0-ce docker-compose 版本為 1.21.0 Python 版本為 2.7.12 安裝 HarborHarbor 安裝主要分為以下三個步驟： 下載 Installer； 設定 harbor.cfg； 執行 run.sh 開始安裝與啟動 Harbor。 下載 InstallerInstaller 的 binary 檔可以從 release page 下載，選擇使用的是 Online Installer 或 Offline Installer，下載完成後，使用tar將 package 解壓縮。 Online Installer：$ tar xvf harbor-online-installer-&lt;version&gt;.tgz Offline Installer：$ tar xvf harbor-offline-installer-&lt;version&gt;.tgz 設定 HarborHarbor 所有的設定與參數都在harbor.cfg中。 harbor.cfg中的參數分為required parameters與optional parameters Required parameters：這類的參數是必須設定的，且會影響使用者更新harbor.cfg後，重新執行安裝腳本來重新安裝 Harbor。 Optional parameters：這類的參數為使用者自行決定是否設定，且只會在第一次安裝時，這些參數的配置才會生效。而 Harbor 啟動後，可以透過 Web UI 進行修改。 而最基本的安裝僅需修改harbor.cfg中的hostname參數，將其更改為自己的hostname即可。 參數設定本文件不再詳述，若想要了解請參考以下網址： Required parameters Optional parameters Configuring storage backend (optional)預設的情況下，Harbor 會將 Docker image 儲存在本機的檔案系統上，在生產環境中，您可以考慮使用其他 storage backend 而不是本機的檔案系統，像是 S3, OpenStack Swift, Ceph 等。而僅需更改common/templates/registry/config.yml。以下為一個接 OpenStack Swift 的範例：storage: swift: username: admin password: ADMIN_PASS authurl: http://keystone_addr:35357/v3/auth tenant: admin domain: default region: regionOne container: docker_images 更多 storage backend 的資訊，請參考 Registry Configuration Reference 執行安裝腳本一旦harbor.cfg設定完成後，可以透過install.sh腳本開始安裝 Harbor。Harbor 有整合Notary與Clair服務，但以下指令僅會安裝一個最基本的 Harbor 並不會安裝Notary與Clair服務：$ sudo ./install.sh 若要安裝 Clair 與 Harbor，則在 install.sh 後面加上一個參數：$ sudo ./install.sh --with-clair Online Installer 會從 Docker hub 下載 Harbor 所需的映像檔，因此會花較久的時間。 如果安裝過程正常，您可以打開瀏覽器並輸入在harbor.cfg中設定的hostname，來存取 Harbor 的 Web UI。 預設的管理者帳號密碼為 admin/Harbor12345 開始使用 Harbor登入成功後，可以創建一個新的 Project，並使用 Docker command 進行登入，但在登入之前，需要對 Docker daemon 新增--insecure-registry參數。新增--insecure-registry參數至/etc/default/docker中：DOCKER_OPTS=\"--insecure-registry &lt;your harbor.cfg hostname&gt;\" 若在 Ubuntu 16.04 版本，還需要修改/lib/systemd/system/docker.service檔案：EnvironmentFile=-/etc/default/%pExecStart=/usr/bin/dockerd -H fd:// $DOCKER_OPTS 其他細節，請參考 Test an insecure registry 修改完成後，重新啟動服務：$ sudo systemctl daemon-reload$ sudo systemctl restart docker.service 登入 Docker client服務重啟成功後，透過 Docker command 進行 login：$ docker login &lt;your harbor.cfg hostname&gt; Push Image將映像檔上 tag 之後，上傳至 Harbor：$ docker tag ubuntu:16.04 &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubuntu:16.04$ docker push &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 可以在 Harbor Web UI 上看到剛剛 push 的 docker image。 Pull Image$ docker pull &lt;your harbor.cfg hostname&gt;/&lt;your project&gt;/ubunut:16.04 更多使用者操作，請參考 Harbor User Guide 參考資料 Harbor Github","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Container","slug":"技術/Container","permalink":"http://ellis-wu.github.io/categories/技術/Container/"}],"tags":[{"name":"Harbor","slug":"Harbor","permalink":"http://ellis-wu.github.io/tags/Harbor/"},{"name":"Docker Registry","slug":"Docker-Registry","permalink":"http://ellis-wu.github.io/tags/Docker-Registry/"}]},{"title":"基於 Kubernetes 的 Serverless 框架 Kubeless","slug":"kubeless-installation","date":"2018-05-01T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2018/05/02/kubeless-installation/","link":"","permalink":"http://ellis-wu.github.io/2018/05/02/kubeless-installation/","excerpt":"Kubeless 是一個基於 Kubernetes 的無伺服器(Serverless)框架，由 Go 語言所撰寫而成。Kubeless 利用 Kubernetes 資源提供自動擴展(auto-scaling)、API 路由、監控以及故障排除等功能。 而 Kubeless 為什麼能在如此多的無伺服器(Serverless)框架中脫穎而出呢？這是因為 Kubeless 使用 Kubernetes 的 CRD(Custom Resource Definition) 來建立 Function，而每個 Function 建立時都會是一個 Deployment，並同時暴露(expose)出一個 Service，透過 Kubernetes 的好處省去開發者開發上的麻煩。而 Kubeless 僅需執行一個in-cluster的 Controller 來監控這些 CRD，並按需求啟動與執行。","text":"Kubeless 是一個基於 Kubernetes 的無伺服器(Serverless)框架，由 Go 語言所撰寫而成。Kubeless 利用 Kubernetes 資源提供自動擴展(auto-scaling)、API 路由、監控以及故障排除等功能。 而 Kubeless 為什麼能在如此多的無伺服器(Serverless)框架中脫穎而出呢？這是因為 Kubeless 使用 Kubernetes 的 CRD(Custom Resource Definition) 來建立 Function，而每個 Function 建立時都會是一個 Deployment，並同時暴露(expose)出一個 Service，透過 Kubernetes 的好處省去開發者開發上的麻煩。而 Kubeless 僅需執行一個in-cluster的 Controller 來監控這些 CRD，並按需求啟動與執行。 事情準備與安裝環境本文事先準備了一個 Kubernetes 叢集，且是利用kubeadm所部署而成；環境並非實體機而為虛擬機機。 IP Address Role CPU RAM Disk 172.20.3.19 Master1 1 vCPU 2 GB 40 GB 172.20.3.14 Node1 1 vCPU 2 GB 40 GB 172.20.3.18 Node2 1 vCPU 2 GB 40 GB Docker 版本為 1.18.02； Kubernetes 版本為 1.10.0；Kubernetes CNI 為Calico。 利用kubeadm部署 Kubernetes 叢集，可以參考 只要用 kubeadm 小朋友都能部署 Kubernetes。 安裝 Kubeless這裡官方為了不同的 Kubernetes 環境提供了不同的 Manifest，請依照自己的需求而選擇對應的檔案： kubeless-$RELEASE.yaml：使用於有 RBAC 的 Kubernetes 叢集； kubeless-non-rbac-$RELEASE.yaml：使用於沒有 RBAC 的 Kubernetes 叢集； kubeless-openshift-$RELEASE.yaml：使用於 OpenShift (1.5+) 上。 設定 Kubeless 版本並選擇對應的檔案後，透過kubectl建立 Kubeless controller manager 與 CRD：$ export RELEASE=$(curl -s https://api.github.com/repos/kubeless/kubeless/releases/latest | grep tag_name | cut -d '\"' -f 4)$ kubectl create ns kubeless$ kubectl create -f https://github.com/kubeless/kubeless/releases/download/$RELEASE/kubeless-non-rbac-$RELEASE.yaml$ kubectl get pods -n kubelessNAME READY STATUS RESTARTS AGEkubeless-controller-manager-6f59c58ffd-mlbpd 1/1 Running 0 6h$ kubectl get deployment -n kubelessNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEkubeless-controller-manager 1 1 1 1 6h$ kubectl get crdNAME AGEcronjobtriggers.kubeless.io 6hfunctions.kubeless.io 6hhttptriggers.kubeless.io 6h 完成後，安裝 Kubeless 的 CLI：$ export OS=$(uname -s| tr '[:upper:]' '[:lower:]')$ curl -OL https://github.com/kubeless/kubeless/releases/download/$RELEASE/kubeless_$OS-amd64.zip &amp;&amp; \\ unzip kubeless_$OS-amd64.zip &amp;&amp; \\ sudo mv bundles/kubeless_$OS-amd64/kubeless /usr/local/bin/ 若環境無法unzip請自行透過apt安裝zip。指令如下：$ apt-get install -y zip 使用kubeless指令檢查 Kubeless CLI 是否安裝完成：$ kubelessServerless framework for KubernetesUsage: kubeless [command]Available Commands: autoscale manage autoscale to function on Kubeless completion Output shell completion code for the specified shell. function function specific operations get-server-config Print the current configuration of the controller help Help about any command topic manage message topics in Kubeless trigger trigger specific operations version Print the version of KubelessFlags: -h, --help help for kubelessUse \"kubeless [command] --help\" for more information about a command. 安裝 Kubeless UIKubeless 提供了 Dashboard，且由 React 所撰寫而成。而安裝方式非常簡單：$ kubectl create -f https://raw.githubusercontent.com/kubeless/kubeless-ui/master/k8s.yaml 在 Dashboard 上會遇到 RBAC 的問題，因此需要建立一個 Cluster Role：$ cat &lt;&lt;EOF | kubectl create -f -apiVersion: rbac.authorization.k8s.io/v1beta1kind: ClusterRoleBindingmetadata: name: kubeless-ui-defaultroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: - kind: User name: system:serviceaccount:kubeless:ui-acct apiGroup: rbac.authorization.k8s.ioEOF 確認 Kubeless UI 的 Service：$ kubectl -n kubeless get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEui NodePort 10.106.89.123 &lt;none&gt; 3000:30619/TCP 6h 若完成後，可以透過瀏覽器存取 Kubeless UI 寫一個簡單的 Function接下來建立一個簡單的 FUnction，以 Python 為範例。 創建一個test.py，並輸入以下內容：def hello(event, context): print event return event['data'] 接下來透過 Kubeless 指令建立 Function：$ kubeless function deploy hello --runtime python2.7 \\ --from-file test.py \\ --handler test.helloINFO[0000] Deploying function...INFO[0000] Function hello submitted for deploymentINFO[0000] Check the deployment status executing 'kubeless function ls hello' 可以透過以下指令確認 Function 是否建立：$ kubeless function lsNAME NAMESPACE HANDLER RUNTIME DEPENDENCIES STATUShello default test.hello python2.7 1/1 READY$ kubectl get functionsNAME AGEhello 52s$ kubectl get poNAME READY STATUS RESTARTS AGEhello-56d89fcd87-4hlh5 1/1 Running 0 1m 完成後，可以在 Kubeless 上看到剛剛建立的 Function。 測試 Function以下有三種方式可以測試剛剛建立的 Function 是否有作用。 Kubeless CLI透過 Kubeless CLI 確認 Function 是否有作用：$ kubeless function call hello --data 'Hello world!'Hello world! Curl直接使用Curl指令 apiserver proxy URL：$ kubectl proxy -p 8080 &amp;$ curl --data '&#123;\"hello\": \"world\"&#125;' \\ --header \"Content-Type:application/json\" \\ localhost:8080/api/v1/namespaces/default/services/hello:8080/proxy/&#123;\"hello\": \"world\"&#125; Kubeless UIKubeless UI 在 Function 旁邊有提供測試。 參考資料 Kubeless Github Kubeless Docs Kubeless UI Github","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Kubernetes","slug":"技術/Kubernetes","permalink":"http://ellis-wu.github.io/categories/技術/Kubernetes/"}],"tags":[{"name":"Kubeless","slug":"Kubeless","permalink":"http://ellis-wu.github.io/tags/Kubeless/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://ellis-wu.github.io/tags/Kubernetes/"},{"name":"Serverless","slug":"Serverless","permalink":"http://ellis-wu.github.io/tags/Serverless/"}]},{"title":"NVIDIA Docker v2 安裝","slug":"nvidia-docker-installation","date":"2018-03-01T16:00:00.000Z","updated":"2019-07-22T02:21:17.798Z","comments":true,"path":"2018/03/02/nvidia-docker-installation/","link":"","permalink":"http://ellis-wu.github.io/2018/03/02/nvidia-docker-installation/","excerpt":"2017 年 11 月 NVIDIA 已將 NVIDIA Docker v2 的版本合併(merged)至 NVIDIA/nvidia-docker 的 repository，這意味著 v2 會逐漸取代 v1。 而根據官方的說明，v1 與 v2 差異如下： 不需要封裝的 Docker CLI 以及獨立的背景程式(daemon) GPU 的隔離現在透過環境變數NVIDIA_VISIBLE_DEVICES 任何的 Docker image 都可以啟動 GPU 支援，不只是基於官方的 CUDA image Ubuntu 與 CentOS 都有 Package repositories，意味著安裝更為簡單與方便 基於libnvidia-container來重新實現 而本文將介紹如何使用 NVIDIA Docker v2 來讓容器使用 GPU。","text":"2017 年 11 月 NVIDIA 已將 NVIDIA Docker v2 的版本合併(merged)至 NVIDIA/nvidia-docker 的 repository，這意味著 v2 會逐漸取代 v1。 而根據官方的說明，v1 與 v2 差異如下： 不需要封裝的 Docker CLI 以及獨立的背景程式(daemon) GPU 的隔離現在透過環境變數NVIDIA_VISIBLE_DEVICES 任何的 Docker image 都可以啟動 GPU 支援，不只是基於官方的 CUDA image Ubuntu 與 CentOS 都有 Package repositories，意味著安裝更為簡單與方便 基於libnvidia-container來重新實現 而本文將介紹如何使用 NVIDIA Docker v2 來讓容器使用 GPU。 安裝環境環境資訊： Name Spec OS Ubunut 16.04 LTS CPU 4 CPU RAM 16 GB Disk 1 TB 事情準備安裝前需要確認以下幾點： GNU/Linux x86_64 with kernel version &gt; 3.10 Docker &gt;= 1.12 NVIDIA GPU with Architecture &gt; Fermi (2.1) NVIDIA drivers ~= 361.93 (untested on older versions) Docker 版本要安裝 stable release 的版本，請參考 這裡。 安裝 Docker CE首先，需要在環境中安裝Docker CE，若使用 Docker 官方提供的快速安裝腳本的話，安裝的Docker CE版本並非 stable 的版本。所以 nvidia-docker 可能並未支援。因此，若要安裝 stable 的 Docker CE 請參考 Docker CE 安裝，這裡不再詳述如何安裝 Docker CE。 安裝 nvidia-dockerDocker CE 安裝完成後，接著可以開始安裝nvidia-docker2：$ apt-get install nvidia-384$ docker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -f$ apt-get purge -y nvidia-docker$ curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -$ curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu16.04/amd64/nvidia-docker.list | \\ tee /etc/apt/sources.list.d/nvidia-docker.list$ apt-get update$ apt-get install -y nvidia-docker2$ pkill -SIGHUP dockerd 安裝完成後，使用nvidia-docker指令確認是否安裝成功：nvidia-docker versionNVIDIA Docker: 2.0.3Client: Version: 18.03.1-ce API version: 1.37 Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:17:20 2018 OS/Arch: linux/amd64 Experimental: false Orchestrator: swarmServer: Engine: Version: 18.03.1-ce API version: 1.37 (minimum version 1.12) Go version: go1.9.5 Git commit: 9ee9f40 Built: Thu Apr 26 07:15:30 2018 OS/Arch: linux/amd64 Experimental: false 驗證 nvidia-docker下載官方的 Docker image，並使用nvidia-smi指令來驗證在 Docker Container 中有抓到 GPU 資訊：$ nvidia-docker run --runtime=nvidia --rm nvidia/cuda nvidia-smiTue Mar 2 06:19:09 2018+-----------------------------------------------------------------------------+| NVIDIA-SMI 384.111 Driver Version: 384.111 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. ||===============================+======================+======================|| 0 GeForce GTX 106... Off | 00000000:01:00.0 Off | N/A || 0% 35C P0 15W / 120W | 0MiB / 3013MiB | 2% Default |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| Processes: GPU Memory || GPU PID Type Process name Usage ||=============================================================================|| No running processes found |+-----------------------------------------------------------------------------+ 參考資料 NVIDIA Docker Repository NVIDIA-Docker version 2 released","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Container","slug":"技術/Container","permalink":"http://ellis-wu.github.io/categories/技術/Container/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://ellis-wu.github.io/tags/Docker/"},{"name":"NVIDIA","slug":"NVIDIA","permalink":"http://ellis-wu.github.io/tags/NVIDIA/"}]},{"title":"在 Ubuntu 上安裝 Docker CE","slug":"docker-installation","date":"2018-02-28T16:00:00.000Z","updated":"2019-07-22T02:21:17.796Z","comments":true,"path":"2018/03/01/docker-installation/","link":"","permalink":"http://ellis-wu.github.io/2018/03/01/docker-installation/","excerpt":"在安裝 Docker CE 時，官方在 get.docker.com 與 test.docker.com 提供一個很方便的腳本來進行安裝。但若要在 production 環境使用此安裝腳本是不建議的，因為它具有以下風險： 使用快速安裝腳本需要使用到root或sudo特權來執行，因此在執行腳本之前你需要特別檢查與確認是否使用； 快速安裝腳本會自動偵測你的 Linux 發行版與版本，並為你配置 package management 系統。此外，腳本不允許設定任何安裝參數； 快速安裝腳本會安裝所有的相依性與推薦的套件，且不需要經過確認； 快速安裝腳本不提供選項來指定安裝 Docker 的版本，它會安裝最新的版本(非 stable release 版本)； 如果主機已經安裝了 Docker 請不要使用快速安裝腳本。 因此，官方建議使用以下方式來安裝 Docker CE。","text":"在安裝 Docker CE 時，官方在 get.docker.com 與 test.docker.com 提供一個很方便的腳本來進行安裝。但若要在 production 環境使用此安裝腳本是不建議的，因為它具有以下風險： 使用快速安裝腳本需要使用到root或sudo特權來執行，因此在執行腳本之前你需要特別檢查與確認是否使用； 快速安裝腳本會自動偵測你的 Linux 發行版與版本，並為你配置 package management 系統。此外，腳本不允許設定任何安裝參數； 快速安裝腳本會安裝所有的相依性與推薦的套件，且不需要經過確認； 快速安裝腳本不提供選項來指定安裝 Docker 的版本，它會安裝最新的版本(非 stable release 版本)； 如果主機已經安裝了 Docker 請不要使用快速安裝腳本。 因此，官方建議使用以下方式來安裝 Docker CE。 安裝 Docker CE首先，更新apt package 並安裝一些相關套件：$ sudo apt-get update$ sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-common 新增Docker的 GPG 金鑰(key)：$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 確認金鑰(key)且驗證金鑰為9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88：$ sudo apt-key fingerprint 0EBFCD88pub 4096R/0EBFCD88 2017-02-22 Key fingerprint = 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88uid Docker Release (CE deb) &lt;docker@docker.com&gt;sub 4096R/F273FCD8 2017-02-22 設定 apt repository：$ sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" 更新 apt package：$ sudo apt-get update 使用以下指令列出docker-ce可以使用的版本：$ sudo apt-cache madison docker-ce docker-ce | 18.03.1~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 18.03.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.12.1~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.12.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.09.1~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.09.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.06.2~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.06.1~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.06.0~ce-0~ubuntu | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.03.2~ce-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://download.docker.com/linux/ubuntu xenial/stable amd64 Packages 選擇其中一個版本並安裝：$ sudo apt-get install docker-ce=17.12.0~ce-0~ubuntu 安裝完成後，可以使用以下指令確認Docker版本：$ sudo docker versionClient: Version: 17.12.0-ce API version: 1.35 Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:11:19 2017 OS/Arch: linux/amd64Server: Engine: Version: 17.12.0-ce API version: 1.35 (minimum version 1.12) Go version: go1.9.2 Git commit: c97c6d6 Built: Wed Dec 27 20:09:53 2017 OS/Arch: linux/amd64 Experimental: false 這時候發現下任何 Docker 指令都需要加sudo，這邊我們可以將使用者加入 Docker 群組：$ sudo usermod -aG docker $USER 之後退出終端畫面再重新連線即可。 參考資料 Get Docker CE for Ubuntu","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Container","slug":"技術/Container","permalink":"http://ellis-wu.github.io/categories/技術/Container/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://ellis-wu.github.io/tags/Docker/"},{"name":"Ubuntu","slug":"Ubuntu","permalink":"http://ellis-wu.github.io/tags/Ubuntu/"}]},{"title":"Kubernetes on Mesos","slug":"kubernetes-on-mesos","date":"2017-11-26T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2017/11/27/kubernetes-on-mesos/","link":"","permalink":"http://ellis-wu.github.io/2017/11/27/kubernetes-on-mesos/","excerpt":"Kubernetes 是一個基於容器技術的分散式架構解決方案，並且是 Google 十幾年大規模使用容器技術的經驗累積的成果。因此，在今年七月 Mesosphere 宣布與 Google 合作，將 Kubernetes 結合 Mesos，讓使用者能夠使用 Kubernetes 與其他一流的 data center 服務(例如：Hadoop、Spark 以及 Chronos)一起使用。這允許 Kubernetes 應用程式同時與其他類型的應用程式在同一組伺服器中一起執行，而 Mesos 可以確保資源被公平的分配且隔離每個應用程式。 Kubernetes-Mesos 目前屬於 alpha 階段，仍然還在開發中，並不建議使用在生產環境。請參考 這裏。","text":"Kubernetes 是一個基於容器技術的分散式架構解決方案，並且是 Google 十幾年大規模使用容器技術的經驗累積的成果。因此，在今年七月 Mesosphere 宣布與 Google 合作，將 Kubernetes 結合 Mesos，讓使用者能夠使用 Kubernetes 與其他一流的 data center 服務(例如：Hadoop、Spark 以及 Chronos)一起使用。這允許 Kubernetes 應用程式同時與其他類型的應用程式在同一組伺服器中一起執行，而 Mesos 可以確保資源被公平的分配且隔離每個應用程式。 Kubernetes-Mesos 目前屬於 alpha 階段，仍然還在開發中，並不建議使用在生產環境。請參考 這裏。 Kubernetes-Mesos 架構Apache Mesos 叢集是由一個或多個 Master 與一個或多個 Slave 所組成，而 Kubernetes-Mesos(K8sm) 為一個 Mesos Framework 且執行在 Mesos 之上。K8sm 提供了兩個元件且連接了 Mesos 與 Kubernetes： Scheduler：整合了 Kubernetes scheduling API 以及 Mesos scheduler runtime。 Executor：整合了 Kubernetes kubelet 服務與 Mesos executor runtime。 當一個 pod 建立透過 Kubernetes 時，K8sm scheduler 會建立一個相關的 Mesos task 並將其排入佇列並進行調度，在依照 pod/task 所需要的資源，將其分配至適合的節點之上，接著 pod/task 將會被啟動並交由 executor。當 executor 啟動 pod/task 時，會透過 kubelet 註冊 pod 並開始由 kubelet 管理 pod 的生命週期。 安裝 Kubernetes on Mesos接下來，將介紹如何在安裝 Kubernetes on Mesos。並執行一個 nginx web server 的 pod。 事前準備 準備一個 Mesos 叢集環境 叢集中選擇一台作為 Kubernetes Master 節點，且需要以下套件： Go (Go 語言安裝版本請參考:Kubernetes Development Guide) make (例如：build-essential) 每台節點都需要安裝 Docker 您可將 Kubernetes-Mesos 部署至與 Mesos Master 同一節點，也可以在不同節點。 部署 Kubernetes-Mesos首先，先選擇一台您要安裝 Kubernetes-Mesos 的節點，並且 build Kubernetes-Mesos：$ git clone https://github.com/kubernetes-incubator/kube-mesos-framework$ cd kube-mesos-framework$ make 設定一些環境變數：$ export KUBERNETES_MASTER_IP=172.22.132.22$ export KUBERNETES_MASTER=http://$&#123;KUBERNETES_MASTER_IP&#125;:8888 部署 etcd透過 Docker 快速啟動一個 etcd 服務，並且驗證他們是否執行：$ docker run -d --hostname $(uname -n) --name etcd \\ -p 4001:4001 -p 7001:7001 quay.io/coreos/etcd:v2.2.1 \\ --listen-client-urls http://0.0.0.0:4001 \\ --advertise-client-urls http://$&#123;KUBERNETES_MASTER_IP&#125;:4001 $ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe2afe13e2319 quay.io/coreos/etcd:v2.2.1 \"/etcd --listen-cl...\" 3 days ago Up 3 days 0.0.0.0:4001-&gt;4001/tcp, 2379-2380/tcp, 0.0.0.0:7001-&gt;7001/tcp etcd 以下方法也是個不錯方法，來測試你的 etcd 是否正常運作：$ curl -L http://$&#123;KUBERNETES_MASTER_IP&#125;:4001/v2/keys/ 如果連線正常，您將會看到 etcd 的輸出結果在 console 上。 啟動 Kubernetes-Mesos 服務更新您的PATH，能更容易執行 Kubernetes-Mesos binary：$ export PATH=\"$(pwd)/_output/local/go/bin:$PATH\" 確定您的 Mesos Master。而這取決於 Mesos 的安裝方式；可能是host:port像是mesos-master:5050或者是 ZooKeeper URL 像是zk://zookeeper:2181/mesos：$ epxort MESOS_MASTER=&lt;host:port or zk://url&gt; 而為了讓 Kubernetes 能夠在 Mesos Master 變更時還能正常運作，建議在生產環境中使用 ZooKeeper URL。 建立一個 cloud config 名為mesos-cloud.conf在當前目錄，並輸入以下內容：$ cat &lt;&lt;EOF &gt;mesos-cloud.conf[mesos-cloud] mesos-master = $&#123;MESOS_MASTER&#125;EOF 現在，啟動 kubernetes-mesos API server、controller manager 以及 scheduler 在 master 節點：$ km apiserver \\ --address=$&#123;KUBERNETES_MASTER_IP&#125; \\ --etcd-servers=http://$&#123;KUBERNETES_MASTER_IP&#125;:4001 \\ --service-cluster-ip-range=10.10.10.0/24 \\ --port=8888 \\ --cloud-provider=mesos \\ --cloud-config=mesos-cloud.conf \\ --secure-port=0 \\ --v=1 &gt;apiserver.log 2&gt;&amp;1 &amp;$ km controller-manager \\ --master=$&#123;KUBERNETES_MASTER&#125; \\ --cloud-provider=mesos \\ --cloud-config=./mesos-cloud.conf \\ --v=1 &gt;controller.log 2&gt;&amp;1 &amp;$ km scheduler \\ --address=$&#123;KUBERNETES_MASTER_IP&#125; \\ --mesos-master=$&#123;MESOS_MASTER&#125; \\ --etcd-servers=http://$&#123;KUBERNETES_MASTER_IP&#125;:4001 \\ --mesos-user=root \\ --api-servers=$&#123;KUBERNETES_MASTER&#125; \\ --cluster-dns=10.10.10.10 \\ --cluster-domain=cluster.local \\ --v=2 &gt;scheduler.log 2&gt;&amp;1 &amp; 這些服務都會跑在背景，如果當 logout 時想要終止掉：$ disown -a 驗證 KM 服務透過kubectl與kubernetes-mesos framework互動： $ kubectl get podsNAME READY STATUS RESTARTS AGEs $ kubectl get servicesNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEk8sm-scheduler ClusterIP 10.10.10.158 &lt;none&gt; 10251/TCP 2dkubernetes ClusterIP 10.10.10.1 &lt;none&gt; 443/TCP 3d 執行一個 POD建立並編輯一個 pod 的 yaml 檔：$ cat &lt;&lt;EOPOD &gt;nginx.yamlapiVersion: v1kind: Podmetadata: name: nginxspec: containers: - name: nginx image: nginx ports: - containerPort: 80EOPOD 使用kubectl指令來建立 nginx pod:$ kubectl create -f ./nginx.yamlpod \"nginx\" created 使用kubectl指令觀察 pod 的是否執行：$ kubectl get podsNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 8m 參考資料 Kubernetes Docs - Kubernetes on Mesos Kubernetes incubator Github Mesosphere Github","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Kubernetes","slug":"技術/Kubernetes","permalink":"http://ellis-wu.github.io/categories/技術/Kubernetes/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://ellis-wu.github.io/tags/Docker/"},{"name":"Kubernetes","slug":"Kubernetes","permalink":"http://ellis-wu.github.io/tags/Kubernetes/"},{"name":"Mesos","slug":"Mesos","permalink":"http://ellis-wu.github.io/tags/Mesos/"}]},{"title":"OpenVPN 手把手教學","slug":"openvpn-installation","date":"2017-03-30T16:00:00.000Z","updated":"2019-07-22T02:21:17.798Z","comments":true,"path":"2017/03/31/openvpn-installation/","link":"","permalink":"http://ellis-wu.github.io/2017/03/31/openvpn-installation/","excerpt":"虛擬私人網路(Virtual Private Network, VPN)為了解決跨區域之大型企業的網路連線與安全問題而衍生，VPN 在公用的網路架構(如：網際網路)上建立私有通道，而訊息透過私有通透進行傳送。VPN 可以利用已加密的通道協議(Tunneling Protocol)來達到保密、傳送端認證、訊息準確性等私人訊息安全效果。這種技術可以用不安全的網路(如：網際網路)來傳送可靠、安全的訊息。需要注意的是，沒有加密的 VPN 訊息依然有被竊取的危險。而 OpenVPN 無疑是 Linux 下開源 VPN 的先鋒，其提供了良好的效能友善的 GUI，而本文將介紹如何在 Ubuntu 上安裝 OpenVPN，但不使用 GUI 介面。","text":"虛擬私人網路(Virtual Private Network, VPN)為了解決跨區域之大型企業的網路連線與安全問題而衍生，VPN 在公用的網路架構(如：網際網路)上建立私有通道，而訊息透過私有通透進行傳送。VPN 可以利用已加密的通道協議(Tunneling Protocol)來達到保密、傳送端認證、訊息準確性等私人訊息安全效果。這種技術可以用不安全的網路(如：網際網路)來傳送可靠、安全的訊息。需要注意的是，沒有加密的 VPN 訊息依然有被竊取的危險。而 OpenVPN 無疑是 Linux 下開源 VPN 的先鋒，其提供了良好的效能友善的 GUI，而本文將介紹如何在 Ubuntu 上安裝 OpenVPN，但不使用 GUI 介面。 安裝環境本次安裝在 Google Compute Engine(GCE) 上租用一台虛擬機並作為 VPN Server，在實體機上使用 KVM 建立一台虛擬機作為 VPN Client。硬體規格與環境資訊如下： ID VPN Server (GCE) VPN Client (On-premises) OS Ubuntu 16.04 LTS Server Ubuntu 16.04 LTS Server vCPU 1 vCPU 1 vCPU RAM 4 GB 4 GB Disk 10 GB 50 GB Private IP ens4, 10.140.0.3 enp0s8, 172.20.3.19 Public IP 35.194.174.185 - 以下 OpenVPN 安裝步驟僅適用於 Ubuntu，其他作業系統如何安裝 OpenVPN 請參考： How To Set Up an OpenVPN Server on Debian 8 Hot To Setup and Configure an OpenVPN Server on CentOS 7 以上連結都未使用 ccd(client-config-dir)，Server 端無法連到 Client 端。因此，需要再額外設定 ccd。 安裝 OpenVPN安裝套件首先，安裝openvpn與easy-rsa套件：$ sudo apt-get install -y openvpn easy-rsa 建立 CA 範本與資料夾接下來，複製easy-rsa的範本：$ make-cadir ~/openvpn-ca$ cd ~/openvpn-ca$ lsbuild-ca build-key build-key-server clean-all openssl-0.9.6.cnf pkitool varsbuild-dh build-key-pass build-req inherit-inter openssl-0.9.8.cnf revoke-full whichopensslcnfbuild-inter build-key-pkcs12 build-req-pass list-crl openssl-1.0.0.cnf sign-req 設定 CA 資訊複製完成並確認檔案後，編輯vars，並設定 CA 資訊：export KEY_COUNTRY=\"TW\"export KEY_PROVINCE=\"Taiwan\"export KEY_CITY=\"Taipei\"export KEY_ORG=\"inwinSTACK\"export KEY_EMAIL=\"ellis.w@inwinstack.com\"export KEY_OU=\"R&amp;D\"# X509 Subject Fieldexport KEY_NAME=\"server\" 建立 CA根據剛剛設定的 CA 資訊來建立自己的憑證：$ cd ~/openvpn-ca$ source varsNOTE: If you run ./clean-all, I will be doing a rm -rf on /home/ellis_w/openvpn-ca/keys 接下來，清除目錄中所有的金鑰(key)，確保等下產生的金鑰(key)不會重複等問題：$ ./clean-all 清楚完後，使用./build-ca來製作 CA，接下來只要一直按 【Enter】 即可：$ ./build-caGenerating a 2048 bit RSA private key............................+++.......................................................................................................................................................+++writing new private key to 'ca.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [TW]:State or Province Name (full name) [Taiwan]:Locality Name (eg, city) [Taipei]:Organization Name (eg, company) [inwinSTACK]:Organizational Unit Name (eg, section) [R&amp;D]:Common Name (eg, your name or your server\\'s hostname) [inwinSTACK CA]:Name [server]:Email Address [ellis.w@inwinstack.com]: 建立 Server 端相關檔案接下來，接下來製作 Server 的 CA 與金鑰(key)：$ ./build-key-server serverGenerating a 2048 bit RSA private key............................+++...........................................+++writing new private key to 'server.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [TW]:State or Province Name (full name) [Taiwan]:Locality Name (eg, city) [Taipei]:Organization Name (eg, company) [inwinSTACK]:Organizational Unit Name (eg, section) [R&amp;D]:Common Name (eg, your name or your server\\'s hostname) [server]:Name [server]:Email Address [ellis.w@inwinstack.com]:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:Using configuration from /home/ellis_w/openvpn-ca/openssl-1.0.0.cnfCheck that the request matches the signatureSignature okThe Subject\\'s Distinguished Name is as followscountryName :PRINTABLE:'TW'stateOrProvinceName :PRINTABLE:'Taiwan'localityName :PRINTABLE:'Taipei'organizationName :PRINTABLE:'inwinSTACK'organizationalUnitName:T61STRING:'R&amp;D'commonName :PRINTABLE:'server'name :PRINTABLE:'server'emailAddress :IA5STRING:'ellis.w@inwinstack.com'Certificate is to be certified .until Mar 29 06:28:48 2027 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base Updated 接著，為了增加 OpenVPN 的安全性，製作diffie hellman。此步驟會需要一些時間：$ ./build-dhGenerating DH parameters, 2048 bit long safe prime, generator 2This is going to take a long .time# ...(以下省略) 建立 Client 端的密鑰(key)Server 端所需要的 CA 與密鑰(key)都已經準備完成後。再來就是建立 Client 端的密鑰(key)。可以在 Client 端產生 Client 的密鑰(key)。但為了方便，直接在 Server 端建立 Client 端的密鑰(key)：$ ./build-key client1Generating a 2048 bit RSA private key................+++...............................................................................................+++writing new private key to 'client1.key'-----You are about to be asked to enter information that will be incorporatedinto your certificate request.What you are about to enter is what is called a Distinguished Name or a DN.There are quite a few fields but you can leave some blankFor some fields there will be a default value,If you enter '.', the field will be left blank.-----Country Name (2 letter code) [TW]:State or Province Name (full name) [Taiwan]:Locality Name (eg, city) [Taipei]:Organization Name (eg, company) [inwinSTACK]:Organizational Unit Name (eg, section) [R&amp;D]:Common Name (eg, your name or your server\\'s hostname) [client1]:Name [server]:Email Address [ellis.w@inwinstack.com]:Please enter the following 'extra' attributesto be sent with your certificate requestA challenge password []:An optional company name []:Using configuration from /home/ellis_w/openvpn-ca/openssl-1.0.0.cnfCheck that the request matches the signatureSignature okThe Subject\\'s Distinguished Name is as followscountryName :PRINTABLE:'TW'stateOrProvinceName :PRINTABLE:'Taiwan'localityName :PRINTABLE:'Taipei'organizationName :PRINTABLE:'inwinSTACK'organizationalUnitName:T61STRING:'R&amp;D'commonName :PRINTABLE:'client1'name :PRINTABLE:'server'emailAddress :IA5STRING:'ellis.w@inwinstack.com'Certificate is to be certified .until Mar 29 06:59:08 2027 GMT (3650 days)Sign the certificate? [y/n]:y1 out of 1 certificate requests certified, commit? [y/n]yWrite out database with 1 new entriesData Base Updated 設定 OpenVPN Server當 Server 與 Client 的 CA 與密鑰(key)都產生完成後，可以開始設定 OpenVPN Server。 複製 Server 需要的 CA 與密鑰(key)至/etc/openvpn/目錄底下：$ cd ~/openvpn-ca/keys$ sudo cp ca.crt ca.key server.crt server.key dh2048.pem /etc/openvpn 接下來，需要準備 OpenVPN 的設定檔。因此解壓縮 OpenVPN 的 sample configuration 檔案，並複製到/etc/openvpn/目錄底下：$ gunzip -c /usr/share/doc/openvpn/examples/sample-config-files/server.conf.gz | sudo tee /etc/openvpn/server.conf 完成後，開始設定 OpenVPN Server 的設定檔，主要修改server.conf中的以下部分，而其他設定資訊請依據各自情況進行設定：# 設定 VPN 透過 8081 Portport 8081# 更改為 tcp 並將 udp 註解起來proto tcp;proto udp# 取消註解並且修改網段push \"route 10.140.0.0 255.255.255.0\"push \"route 172.20.3.0 255.255.255.0\"# 取消註解並寫修改client-config-dir ccdroute 172.20.3.0 255.255.255.0 當server.conf設定好後，新增ccd目錄，並產生client檔案並加入iroute：$ mkdir -p /etc/openvpn/ccd$ echo \"iroute 172.20.3.0 255.255.255.0\" &gt; /etc/openvpn/ccd/client1 調整 OpenVPN Serve 網路設定 此步驟可以依照自己的情況進行更改。 正常情況下會設定防火牆是因為考慮到安全性，倘若有些應用需關閉防火牆，請自行關閉防火牆。 當server.conf都設定完成後，再來要設定網路部分。首先要設定允許 IP forwarding，編輯/etc/sysctl.conf檔案，找到net.ipv4.ip_forward參數，將其取消註解：net.ipv4.ip_forward=1 完成後，退出並存檔。並輸入以下指令，使其重新讀取設定：$ sudo sysctl -p 接下來設定防火牆，而在設定防火牆前，先找到自己主機的 public network interface：$ ip route | grep defaultdefault via 10.140.0.1 dev ens4 這代表我的 public network interface 的名稱為ens4。知道 public network interface 名稱後，編輯/etc/ufw/before.rules，並增加以下設定：# START OPENVPN RULES# NAT table rules*nat:POSTROUTING ACCEPT [0:0]# Allow traffic from OpenVPN client to wlp11s0 (change to the interface you discovered!)-A POSTROUTING -s 10.8.0.0/8 -o ens4 -j MASQUERADECOMMIT# END OPENVPN RULES 再來編輯/etc/default/ufw，將DEFAULT_FORWARD_POLICY的參數設定為ACCEPT：DEFAULT_FORWARD_POLICY=\"ACCEPT\" 接下來，打開 OpenVPN 設定的 Port 與 Protocol，並重新啟動防火牆並啟動設定：$ sudo ufw allow 8081/tcpRules updatedRules updated (v6)$ sudo ufw allow OpenSSHRules updatedRules updated (v6)$ sudo ufw disableFirewall stopped and disabled on system startup$ sudo ufw enableCommand may disrupt existing ssh connections. Proceed with operation (y|n)? yFirewall is active and enabled on system startup 啟動 OpenVPN 服務以上設定完成後，可以啟動 OpenVPN Server，並設定為開機時會自動啟動 OpenVPN Server 服務：$ sudo systemctl start openvpn@server$ sudo systemctl enable openvpn@server 可以使用以下指令查看，是否有成功啟動 OpenVPN Server：$ sudo systemctl status openvpn@server● openvpn@server.service - OpenVPN connection to server Loaded: loaded (/lib/systemd/system/openvpn@.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2018-03-27 08:18:33 UTC; 1h 0min ago Docs: man:openvpn(8) https://community.openvpn.net/openvpn/wiki/Openvpn23ManPage https://community.openvpn.net/openvpn/wiki/HOWTO Process: 30905 ExecStart=/usr/sbin/openvpn --daemon ovpn-%i --status /run/openvpn/%i.status 10 --cd /etc/openvpn --script-security 2 --config /etc/openvpn/%i.conf --writepid /run/openvpn/%i Main PID: 30907 (openvpn) CGroup: /system.slice/system-openvpn.slice/openvpn@server.service └─30907 /usr/sbin/openvpn --daemon ovpn-server --status /run/openvpn/server.status 10 --cd /etc/openvpn --script-security 2 --config /etc/openvpn/server.conf --writepid /run/openvpMar 27 08:27:03 instance-3 ovpn-server[30907]: MULTI: Learn: 172.20.3.19 -&gt; client1/122.146.93.152:26820Mar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 VERIFY OK: depth=1, C=TW, ST=Taiwan, L=Taipei, O=inwinSTACK, OU=R&amp;D, CN=inwinSTACK CA, name=server, emailAddress=elMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 VERIFY OK: depth=0, C=TW, ST=Taiwan, L=Taipei, O=inwinSTACK, OU=R&amp;D, CN=client1, name=server, emailAddress=ellis.w@Mar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 Data Channel Encrypt: Cipher 'BF-CBC' initialized with 128 bit keyMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 WARNING: this cipher\\'s block size is less than 128 bit (64 bit). Consider using a --cipher with a larger block sizMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 Data Channel Encrypt: Using 160 bit message hash 'SHA1' for HMAC authenticationMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 Data Channel Decrypt: Cipher 'BF-CBC' initialized with 128 bit keyMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 WARNING: this cipher\\'s block size is less than 128 bit (64 bit). Consider using a --cipher with a larger block sizMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 Data Channel Decrypt: Using 160 bit message hash 'SHA1' for HMAC authenticationMar 27 09:18:39 instance-3 ovpn-server[30907]: client1/122.146.93.152:26820 Control Channel: TLSv1.2, cipher TLSv1/SSLv3 DHE-RSA-AES256-GCM-SHA384, 2048 bit RSA 可以透過以下指令查看 OpenVPN 建立了一張虛擬網卡tun0：$ ip addr show tun03: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100 link/none inet 10.8.0.1 peer 10.8.0.2/32 scope global tun0 valid_lft forever preferred_lft forever inet6 fe80::7187:f47e:c174:728f/64 scope link flags 800 valid_lft forever preferred_lft forever OpenVPN Client 測試安裝 OpenVPN Client使用 Linux 做為 Client 端，若其他裝置(如：Windows, Mac OS X, Android, iOS)，請參考 How To Set Up an OpenVPN Server on Ubunut 16.04 #Step 12:Install the Client Configuration 當 OpenVPN Server 執行後，再來到 Client 端中安裝 OpenVPN：$ sudo apt-get install -y openvpn 設定 OpenVPN Client首先，Client 會需要用到剛剛在 Server 端建立的ca.crt與 client 密鑰(key)與 CA。因此，我們必須先將這三份文件給複製至 Client 端：$ for file in ca.crt client1.crt client1.key; do sudo scp &lt;username&gt;@&lt;server ip&gt;:/home/&lt;username&gt;/openvpn-ca/keys/$&#123;file&#125; /etc/openvpn/ done Client 所需的密鑰(key)與 CA 複製完成後，接下來複製 Client 的範例文件：$ sudo cp /usr/share/doc/openvpn/examples/sample-config-files/client.conf /etc/openvpn/ 接著開始配置client.conf，主要修改以下部分：# 將 udp 換成 tcpproto tcp;proto udp# 設定 OpenVPN Server IP 與 Portremote 35.194.174.185 8081;remote my-server-2 1194# 找到 ca, cert, key 設定，並修改檔案名稱ca ca.crtcert client1.crtkey client1.key 啟動 OpenVPN Client以上設定完成後，可以啟動 OpenVPN Client，並設定為開機時會自動啟動 OpenVPN Client 服務：$ sudo systemctl start openvpn@client$ sudo systemctl enable openvpn@client 若 OpenVPN Client 成功啟動，則會自動建立一張 tun0 網卡與 Server 連接：$ ip addr show tun04: tun0: &lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 100 link/none inet 10.8.0.6 peer 10.8.0.5/32 scope global tun0 valid_lft forever preferred_lft forever 亦可以使用以下指令查看，OpenVPN Client 是否有任何錯誤資訊：$ sudo systemctl status openvpn@client● openvpn@client.service - OpenVPN connection to client Loaded: loaded (/lib/systemd/system/openvpn@.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2018-03-27 06:30:55 UTC; 2h 50min ago Docs: man:openvpn(8) https://community.openvpn.net/openvpn/wiki/Openvpn23ManPage https://community.openvpn.net/openvpn/wiki/HOWTO Main PID: 21276 (openvpn) CGroup: /system.slice/system-openvpn.slice/openvpn@client.service └─21276 /usr/sbin/openvpn --daemon ovpn-client --status /run/openvpn/client.status 10 --cd /etc/openvpn --script-security 2 --config /etc/openvpn/client.conf --writepid /run/openvpMar 27 09:18:39 single-node1 ovpn-client[21276]: ++ Certificate has EKU (str) TLS Web Server Authentication, expects TLS Web Server AuthenticationMar 27 09:18:39 single-node1 ovpn-client[21276]: VERIFY EKU OKMar 27 09:18:39 single-node1 ovpn-client[21276]: VERIFY OK: depth=0, C=TW, ST=Taiwan, L=Taipei, O=inwinSTACK, OU=R&amp;D, CN=server, name=server, emailAddress=ellis.w@inwinstack.comMar 27 09:18:39 single-node1 ovpn-client[21276]: Data Channel Encrypt: Cipher 'BF-CBC' initialized with 128 bit keyMar 27 09:18:39 single-node1 ovpn-client[21276]: WARNING: this cipher\\'s block size is less than 128 bit (64 bit). Consider using a --cipher with a larger block size.Mar 27 09:18:39 single-node1 ovpn-client[21276]: Data Channel Encrypt: Using 160 bit message hash 'SHA1' for HMAC authenticationMar 27 09:18:39 single-node1 ovpn-client[21276]: Data Channel Decrypt: Cipher 'BF-CBC' initialized with 128 bit keyMar 27 09:18:39 single-node1 ovpn-client[21276]: WARNING: this cipher\\'s block size is less than 128 bit (64 bit). Consider using a --cipher with a larger block size.Mar 27 09:18:39 single-node1 ovpn-client[21276]: Data Channel Decrypt: Using 160 bit message hash 'SHA1' for HMAC authenticationMar 27 09:18:39 single-node1 ovpn-client[21276]: Control Channel: TLSv1.2, cipher TLSv1/SSLv3 DHE-RSA-AES256-GCM-SHA384, 2048 bit RSA 測試連線以上都沒有錯誤，就代表 VPN 以連線成功。可以透過icmp驗證是否有成功連線： 在 Client 端，icmp 至 Server IP Address：$ ping 10.140.0.3 -c 4PING 10.140.0.3 (10.140.0.3) 56(84) bytes of data.64 bytes from 10.140.0.3: icmp_seq=1 ttl=64 time=5.68 ms64 bytes from 10.140.0.3: icmp_seq=2 ttl=64 time=6.14 ms64 bytes from 10.140.0.3: icmp_seq=3 ttl=64 time=5.89 ms64 bytes from 10.140.0.3: icmp_seq=4 ttl=64 time=5.93 ms--- 10.140.0.3 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3007msrtt min/avg/max/mdev = 5.681/5.914/6.143/0.172 ms 在 Server 端，icmp 至 Client IP Address：$ ping 172.20.3.19 -c 4PING 172.20.3.19 (172.20.3.19) 56(84) bytes of data.64 bytes from 172.20.3.19: icmp_seq=1 ttl=64 time=5.50 ms64 bytes from 172.20.3.19: icmp_seq=2 ttl=64 time=5.62 ms64 bytes from 172.20.3.19: icmp_seq=3 ttl=64 time=6.00 ms64 bytes from 172.20.3.19: icmp_seq=4 ttl=64 time=5.57 ms--- 172.20.3.19 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3003msrtt min/avg/max/mdev = 5.507/5.678/6.005/0.213 ms 若以上icmp都能通代表，VPN tunnel 已經成功建立，大功告成！!!","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Linux","slug":"技術/Linux","permalink":"http://ellis-wu.github.io/categories/技術/Linux/"}],"tags":[{"name":"OpenVPN","slug":"OpenVPN","permalink":"http://ellis-wu.github.io/tags/OpenVPN/"},{"name":"VPN","slug":"VPN","permalink":"http://ellis-wu.github.io/tags/VPN/"}]},{"title":"Mac OS X 製作 USB 開機碟","slug":"mac-create-usb-boot","date":"2017-03-16T16:00:00.000Z","updated":"2019-07-22T02:21:17.798Z","comments":true,"path":"2017/03/17/mac-create-usb-boot/","link":"","permalink":"http://ellis-wu.github.io/2017/03/17/mac-create-usb-boot/","excerpt":"本文將介紹如何在 Mac OS X 環境中製作用來安裝作業系統(Operating system)的 USB 隨身碟，即不需要光碟就可以直接安裝作業系統。 請勿用來製作需要授權(license)付費之作業系統，請製作開源作業系統；如：Ubuntu, CentOS 等等。","text":"本文將介紹如何在 Mac OS X 環境中製作用來安裝作業系統(Operating system)的 USB 隨身碟，即不需要光碟就可以直接安裝作業系統。 請勿用來製作需要授權(license)付費之作業系統，請製作開源作業系統；如：Ubuntu, CentOS 等等。 在開始製作前請事先準備一個隨身碟，並確認隨身碟中的資料是不需要的，因為在製作成 USB 開機碟之後，原本隨身碟中的資料都會被刪除。 開始製作 USB 開機碟首先，須先準備作業系統的 ISO 檔，而本文將以Ubuntu 14.04為例；下載的檔案名稱為ubuntu-14.04.2-desktop-amd64。下載好映像檔後，透過hdiutil指令將 ISO 檔轉換為 Max OS X 的 dmg 檔：$ hdiutil convert -format UDRW -o ubuntu-14.04.2-desktop-amd64.dmg ubuntu-14.04.2-desktop-amd64.iso正在讀取Driver Descriptor Map（DDM：0）⋯正在讀取Ubuntu 14.04.2 LTS amd64 （Apple_ISO：1）⋯正在讀取Apple（Apple_partition_map：2）⋯正在讀取Ubuntu 14.04.2 LTS amd64 （Apple_ISO：3）⋯.正在讀取EFI（Apple_HFS：4）⋯.正在讀取Ubuntu 14.04.2 LTS amd64 （Apple_ISO：5）⋯.............................................................................................................................經過時間： 7.604s速度：131.0Mbyte/秒節省：0.0%created: /Users/fengmingwu/Desktop/ubuntu-14.04.2-desktop-amd64.dmg 在 Mac 插入 USB 之前，先使用diskutil確認目前系統的硬碟狀態：$ diskutil list/dev/disk0 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *121.3 GB disk0 1: EFI EFI 209.7 MB disk0s1 2: Apple_CoreStorage Macintosh HD 120.5 GB disk0s2 3: Apple_Boot Recovery HD 650.0 MB disk0s3/dev/disk1 (internal, virtual): #: TYPE NAME SIZE IDENTIFIER 0: Macintosh HD +120.1 GB disk1 Logical Volume on disk0s2 4925F827-81AC-4A28-B0B1-408E6475C692 Unlocked Encrypted/dev/disk2 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *129.8 GB disk2 1: Windows_NTFS jetDrive 129.8 GB disk2s1 插入 USB 之後，在使用diskutil確認目前系統的硬碟狀態：$ diskutil list/dev/disk0 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID_partition_scheme *121.3 GB disk0 1: EFI EFI 209.7 MB disk0s1 2: Apple_CoreStorage Macintosh HD 120.5 GB disk0s2 3: Apple_Boot Recovery HD 650.0 MB disk0s3/dev/disk1 (internal, virtual): #: TYPE NAME SIZE IDENTIFIER 0: Macintosh HD +120.1 GB disk1 Logical Volume on disk0s2 4925F827-81AC-4A28-B0B1-408E6475C692 Unlocked Encrypted/dev/disk2 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: FDisk_partition_scheme *129.8 GB disk2 1: Windows_NTFS jetDrive 129.8 GB disk2s1/dev/disk3 (external, physical): #: TYPE NAME SIZE IDENTIFIER 0: Apple_partition_scheme *7.9 GB disk3 1: Apple_partition_map 4.1 KB disk3s1 2: Apple_HFS 2.2 MB disk3s2 比較一下前後兩個輸出，就可以發現 USB 是/dev/disk3。 這個部分每一台電腦結果不盡相同，請依照自己的輸出來判斷哪個為 USB。 確認後 USB 後，使用diskutil將 USB 卸載：$ diskutil unmountDisk /dev/disk3Unmount of all volumes on disk3 was successful 接者使用dd指令將 dmg 檔的內容寫入 USB：$ sudo dd if=ubuntu-14.04.2-desktop-amd64.dmg of=/dev/rdisk3 bs=1mPassword:996+0 records in996+0 records out1044381696 bytes transferred in 184.816731 secs (5650904 bytes/sec) 接著，退出 USB：$ diskutil eject /dev/disk1 大功告成！ 參考資料 在 Mac OS X 中製作用來安裝 Ubuntu Linux 的 USB 隨身碟","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Mac","slug":"技術/Mac","permalink":"http://ellis-wu.github.io/categories/技術/Mac/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://ellis-wu.github.io/tags/Mac/"}]},{"title":"KVM 虛擬機器掛載硬碟","slug":"kvm-mount-virt-disk","date":"2016-10-15T16:00:00.000Z","updated":"2019-07-22T02:21:17.798Z","comments":true,"path":"2016/10/16/kvm-mount-virt-disk/","link":"","permalink":"http://ellis-wu.github.io/2016/10/16/kvm-mount-virt-disk/","excerpt":"最近在安裝 Ceph 分散式儲存時，需要多顆硬碟，而實體機並沒有這麼多硬碟時，變透過 KVM 來模擬多顆硬碟給虛擬機使用。本文將介紹如何使 KVM 所建立的虛擬機掛載新的虛擬硬碟。若不知道如何安裝 KVM，請參考 。","text":"最近在安裝 Ceph 分散式儲存時，需要多顆硬碟，而實體機並沒有這麼多硬碟時，變透過 KVM 來模擬多顆硬碟給虛擬機使用。本文將介紹如何使 KVM 所建立的虛擬機掛載新的虛擬硬碟。若不知道如何安裝 KVM，請參考 。 新增硬碟進入虛擬機器，使用fdisk指令查看目前硬碟資訊，可以看到只有掛載一顆vda：$ sudo fdisk -lDisk /dev/ram0: 64 MiB, 67108864 bytes, 131072 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytes# [omitting much detailed data here]Disk /dev/vda: 15 GiB, 16106127360 bytes, 31457280 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x895a7464Device Boot Start End Sectors Size Id Type/dev/vda1 * 2048 999423 997376 487M 83 Linux/dev/vda2 1001470 31455231 30453762 14.5G 5 Extended/dev/vda5 1001472 31455231 30453760 14.5G 8e Linux LVMDisk /dev/mapper/test--vg-root: 13.5 GiB, 14516486144 bytes, 28352512 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/test--vg-swap_1: 1 GiB, 1073741824 bytes, 2097152 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 【檢視】接下來，透過virt-manager可以很方便且快速的對虛擬機新增硬體。首先點選要新增硬體的虛擬機，並顯示虛擬機器的詳細規格： 點擊上方工具列的 【檢視】； 點選下拉式表單的 【細節】。 點選 【細節】 後，會列出此虛擬機器的詳細硬體規格及其設定： 透過下方的 【加入硬體】 新增一顆虛擬硬體，並依據自己的需求進行配置： 完成後，進入虛擬機器使用fdisk指令查看硬碟資訊，可以看到已經vdb掛載上去： $ sudo fdisk -lDisk /dev/ram0: 64 MiB, 67108864 bytes, 131072 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 4096 bytesI/O size (minimum/optimal): 4096 bytes / 4096 bytes# [omitting much detailed data here]Disk /dev/vda: 15 GiB, 16106127360 bytes, 31457280 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisklabel type: dosDisk identifier: 0x895a7464Device Boot Start End Sectors Size Id Type/dev/vda1 * 2048 999423 997376 487M 83 Linux/dev/vda2 1001470 31455231 30453762 14.5G 5 Extended/dev/vda5 1001472 31455231 30453760 14.5G 8e Linux LVMDisk /dev/mapper/test--vg-root: 13.5 GiB, 14516486144 bytes, 28352512 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/test--vg-swap_1: 1 GiB, 1073741824 bytes, 2097152 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/vdb: 20 GiB, 21474836480 bytes, 41943040 sectorsUnits: sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes 參考資料 给KVM虚拟机增加硬盘","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Linux","slug":"技術/Linux","permalink":"http://ellis-wu.github.io/categories/技術/Linux/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://ellis-wu.github.io/tags/KVM/"},{"name":"Virtualization","slug":"Virtualization","permalink":"http://ellis-wu.github.io/tags/Virtualization/"}]},{"title":"在 Ubuntu 使用 KVM 建立虛擬機","slug":"using-kvm-on-ubuntu","date":"2016-10-13T16:00:00.000Z","updated":"2019-07-22T02:21:17.799Z","comments":true,"path":"2016/10/14/using-kvm-on-ubuntu/","link":"","permalink":"http://ellis-wu.github.io/2016/10/14/using-kvm-on-ubuntu/","excerpt":"Linux 核心從 2007 年的 2.6.20 版本之後開始支援一種稱為「核心基礎虛擬機器 (Kernel-based Virtual Machine, KVM)」的虛擬化技術。KVM 利用模組化的方式讓 Linux 核心具有 Hypervisor 的能力，因此可以利用 KVM 來建立虛擬機。","text":"Linux 核心從 2007 年的 2.6.20 版本之後開始支援一種稱為「核心基礎虛擬機器 (Kernel-based Virtual Machine, KVM)」的虛擬化技術。KVM 利用模組化的方式讓 Linux 核心具有 Hypervisor 的能力，因此可以利用 KVM 來建立虛擬機。 安裝 KVM 與 virt-manager本安裝教學作業系統為 Desktop 版本，所以利用 GUI 介面來操作 KVM 對虛擬機做任何操作，而透過 GUI 介面對虛擬機做操作也極為方便。若作業系統使用 Server 版本，則無法使用 GUI 介面來操作虛擬機，須透過virsh指令來操作虛擬機。 安裝環境 ID Description OS Ubuntu 16.04 LTS Desktop CPU Intel i3-3220 CPU @ 3.30GHz x 4 RAM 8 GB Disk 500 GB 安裝步驟使用apt進行安裝相關套件：$ sudo apt-get install -y qemu-kvm libvirt-bin ubuntu-vm-builder bridge-utils KVM 相關套件： libvirt-bin：提供 libvirtd，用來管理 qemu 與 kvm。 qemu-kvm：主要的虛擬引擎。 ubuntu-vm-builder：強大的虛擬機器製作工具。 bridge-utils：用來建立虛擬機器使用的 bridge。 安裝完畢後，可以使用kvm-ok來查看是否安裝成功：$ kvm-okINFO: /dev/kvm existsKVM acceleration can be used 安裝虛擬機管理員(Virtual Machine Manager, virt-manager)，利用 GUI 介面對虛擬機進行操作：$ sudo apt-get install virt-manager python-spice-client-gtk 虛擬機管理員會需要用到python-spice-client-gtk 安裝完畢後，即可以開始執行：$ sudo virt-manager 輸入完以上指令後，會開啟虛擬機管理員，再來即可使用 GUI 對虛擬機進行操作。 參考文獻 在 Ubuntu Linux 中使用 KVM（使用 vmbuilder） Ubuntu上装KVM：安装、初次使用 架設 Linux KVM 虛擬化主機（Set up Linux KVM virtualization host）","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Linux","slug":"技術/Linux","permalink":"http://ellis-wu.github.io/categories/技術/Linux/"}],"tags":[{"name":"KVM","slug":"KVM","permalink":"http://ellis-wu.github.io/tags/KVM/"},{"name":"Virtualization","slug":"Virtualization","permalink":"http://ellis-wu.github.io/tags/Virtualization/"}]},{"title":"CEPH 資料分散：CRUSH 演算法與一致性 Hash","slug":"ceph-crush-introduction","date":"2016-10-06T16:00:00.000Z","updated":"2019-07-22T02:21:17.796Z","comments":true,"path":"2016/10/07/ceph-crush-introduction/","link":"","permalink":"http://ellis-wu.github.io/2016/10/07/ceph-crush-introduction/","excerpt":"本篇文章為翻譯文章，原文資訊如下： 原文網址：Ceph剖析：数据分布之CRUSH算法与一致性Hash 原文作者：吳香偉 將資料分散儲存是分散式儲存的一個重要議題，而將資料分散的演算法至少要考慮以下三個因素： 故障與隔離：相同資料的不同副本應分散在不同的故障區域，以降低資料損壞的風險。 負載平衡：資料要能夠平均分散在硬碟容量不同的儲存節點，避免部分節點硬碟空間剩餘過多；而部分節點硬碟空間不足甚至超載，進而影響系統效能。 新增/刪除節點時，所造成的資料搬移：當刪除節點時，最好的資料搬移是只有將離線節點上的資料搬移至其它節點，而其餘節點的資料不會發生搬移。 物件儲存(Object Store)中的一致性 Hash與 Ceph 的CRUSH是比較常見的資料分散演算法。而在 Amazon 的 Dynamo 的 key-value 儲存系統中使用了一致性 Hash，並且對其做了很多優化；OpenStack 的 Swift 物件儲存系統亦使用了一致性 Hash。","text":"本篇文章為翻譯文章，原文資訊如下： 原文網址：Ceph剖析：数据分布之CRUSH算法与一致性Hash 原文作者：吳香偉 將資料分散儲存是分散式儲存的一個重要議題，而將資料分散的演算法至少要考慮以下三個因素： 故障與隔離：相同資料的不同副本應分散在不同的故障區域，以降低資料損壞的風險。 負載平衡：資料要能夠平均分散在硬碟容量不同的儲存節點，避免部分節點硬碟空間剩餘過多；而部分節點硬碟空間不足甚至超載，進而影響系統效能。 新增/刪除節點時，所造成的資料搬移：當刪除節點時，最好的資料搬移是只有將離線節點上的資料搬移至其它節點，而其餘節點的資料不會發生搬移。 物件儲存(Object Store)中的一致性 Hash與 Ceph 的CRUSH是比較常見的資料分散演算法。而在 Amazon 的 Dynamo 的 key-value 儲存系統中使用了一致性 Hash，並且對其做了很多優化；OpenStack 的 Swift 物件儲存系統亦使用了一致性 Hash。 一致性 Hash假設資料為 x，儲存節點數量為 N。將資料分散至儲存節點中，最簡單的方法是： 計算資料 x 的 hash 值； 將 hash(x) % N，其結果即為將資料 x 儲存至節點 N。 而資料經過 Hash 的目的是為了可以讓資料平均分散在 N 個節點中，但這種做法有一個嚴重的問題，就是當加入或刪除節點時，幾乎所有資料都會受到影響，需要重新分配，因此，其資料的搬移量是非常大的。 如上圖所示，一致性 Hash將資料和儲存節點映射到同個 Hash 空間。Hash 環中的 3 個儲存節點把 Hash 空間分為 3 個區段，每個儲存節點負責一個區段上的資料，例如：區段 [N2, N0] 上的資料儲存在節點N0。 一致性 Hash能夠很好的控制節點的新增與刪除所導致的資料搬移的搬移量。 刪除節點如上圖(b)所示，當節點N0刪除時，原本由它負責的 [N2, N0] 區段與 [N0, N1] 區段合併成為 [N2, N1] 區段，並且都由節點N1負責。也就是說本來儲存在節點N0上的資料都搬移至節點N1，而原本儲存在N1與N2節點上的資料不受影響。 新增節點如上圖(c)所示，當節點N3加入時，原本 [N2, N0] 區段分為 [N3, N0] 與 [N2, N3] 兩個區段，其中 [N3, N0] 區段上的資料搬移到新加入的節點N3。 虛擬節點一致性 Hash的一個問題是，儲存節點不能將 Hash 空間平均的劃分。如下圖(a)所示，區段 [N2, N0] 的大小幾乎是其他區段大小之和，這容易讓負責該區段的節點N0負載過重。假設 3 個節點的硬碟容器相等，那麼當節點N0的硬碟容量已經寫滿資料時，其他兩個節點上的硬碟還有很大的空間，但此時系統已經無法繼續向區段 [N2, N0] 寫入資料，而造成資源的浪費。 虛擬節點是相對於實體節點，虛擬節點負責的區段上的資料，最終儲存到其對應的實體節點。在一致性 Hash中引入虛擬節點可以把 Hash 空間劃分成更多區段，讓資料能更平均的分散在儲存節點上。如上圖(b)所示，紅色的節點代表虛擬節點，Ni_0 代表該虛擬節點對應於實體節點 i 的第 0 個虛擬節點。增加虛擬節點後： 實體節點N0負責 [N0, N1_0] 與 [N2_0, N1_0] 兩個區段； 實體節點N1負責 [N0_0, N1] 與 [N0, N0_0] 兩個區段； 實體節點N2負責 [N2, N1] 與 [N2_0, N2] 兩個區段。 因此，三個物理節點負責的資量量趨於平均。 而在實際應用中，可以依據實體節點的硬碟大小來確定其對應的虛擬節點數量。虛擬節點數量越多，節點負責的資料區段也越大。 區段與區段位置先前提到，當節點加入或刪除時，區段會進行分裂或合併，這不對新寫入的資料構成影響，但對於已經寫入到硬碟的資料需要重新計算 Hash 值以確定它是否需要搬移到其他節點，因為需要查詢整個硬碟中的所有資料，這個計算過程非常耗時。 如下圖(a)所示，區段是由落在 Hash 還上的虛擬節點 Ti 來劃分的，並且區段位置(儲存區段資料的節點)也同虛擬節點相關，及儲存到其順時針方向的第 1 個虛擬節點。 在 Dynamo 的論文中提出了分離區段和區段位置的方法來解決這個問題。該方法將 Hash 空間分為固定的若干區段，虛擬節點不再用於劃分區段，而用來確定區段的儲存位置。如上圖(b)所示，將 Hash 空間劃分成 [A, B]、[B, C]、[C, D]、[D, A] 四個固定的區段。虛擬節點用於確定區段位置，例如：T1 負責區段 [B, C]；T2 負責區段 [C, D]；T0 負責區段 [D, A] 和 [A, B] 兩個區段。由於區段固定，因此搬移資料時，可以很容易知道哪些資料需要搬移；哪些資料不需要搬移。 上圖(b)中虛擬節點T0負責了 [D, A] 和 [A, B] 兩個區段的資料，這是由於區段數量與虛擬節點數量不同導致的。而為了讓區段分佈得更加平均，Dynamo 提出了區段數量和虛擬節點數量相等的方法，這樣每個虛擬節點負責一個區段，在實體節點的硬碟容量相同並且虛擬節點數量相同的情況下，每個實體節點負責的區段大小是完全相同的，進而達到最佳的資料分散。 CRUSH 演算法Ceph 分散資料的過程： 計算資料 x 的 hash 值； 將 hash(x) % PG數量，其結果即為將資料 x 儲存至的 pg 編號； 然後，透過CRUSH將 PG 映射到一組 OSD 中； 最後，把資料 x 儲存至 PG 對應的 OSD 中。 這個過程中包含了兩次映射，第一次是資料 x 到 PG 的映射，若把 pg 當作儲存節點，那麼這雨先前提到的一般 Hash 演算法一樣，但 pg 是抽象的儲存節點，它不會隨著實體節點的新增或刪除而增加或減少，因此資料到 pg 的映射是穩定的。 在這個過程中，PG 擁有兩個作用： 劃分資料區段：每個 PG 管理的資料區段大小相同，因此資料能更平均的分散至 PG 上。 充當 Dynamo 中 Token 的角色，即決定區段位置：實際上，這和 Dynamo 中固定區段數量以及維持區段數量和虛擬機節點數量相等的原則是一樣的概念。 在沒有多副本的情況下，Dynamo 中區段的資料直接儲存到 Token，而每個 Token 對應唯一的一個實體儲存節點。而在多副本(假設副本數為 N)的情況下，區段的資料會儲存至連續的 N 個 Token 之中，而這衍生了一個新的問題：因為副本必須保持在不同的實體節點上，但這組 Token 中存在兩個或多個 Token 對應到同一個實體儲存節點，那勢必要跳過這樣的節點。而 Dynamo 使用 Preference 列表來紀錄每個區段對應的實體節點，且論文中沒有詳述區段的 Preference 列表如何選擇實體節點與選擇實體節點時，該如何隔離故障區域等問題。 (osd0, osd1, osd2 … osdn) = CRUSH(x) Ceph 的 PG 擔任起 Dynamo 中 Token、固定區段與 Preference 列表的角色，解決了同樣的問題。PG 的 Acting 集合等同於 Preference 列表，CRUSH解決了 Dynamo 論文中未提及的問題。 OSD 階層架構和權重大小CRUSH的目的是： 為給定的 PG(即區段)分配一組儲存資料的 OSD 節點 而選擇 OSD 節點的過程，要考慮以下幾個因素： PG 在 OSD 間平均分散：假設每個 OSD 的硬碟大小皆相同，那麼我們希望 PG 在每個 OSD 節點上是平均分散的，也就是說每個 OSD 節點包含相同數量的 PG。假如節點的硬碟容量不等，那麼硬碟容量大的節點能夠處理更多數量的 PG。 PG 的 OSD 分佈在不同的故障域：因為 PG 的 OSD 列表用於保存資料在不同的副本，副本分佈在不同的 OSD 中可以降低資料損壞的風險。 Ceph 使用樹型階層架構描述 OS 的空間位置與權重大小。如上圖所示，階層架構描述了 OSD 所在主機、主機所在機架與機架所在的機房等空間位置。這些空間位置隱含了故障區域，例如：使用不同電源的不同機架屬於不同故障區域。CRUSH能夠依據一定的規則將副本放置在不同的故障區域。 OSD 節點在階層架構中也被稱為Device，它位於階層架構的葉節點，所有非葉節點的稱為Bucket。而 Bucket 擁有不同的類型，如上圖所示，所有機架的類型為 Rack；所有主機的類型為 Host，使用者可以自行定義 Bucket 的類型。Device 節點的權重代表儲存節點的效能，而硬碟容量大小是影響權重大小的重要參數。Bucket 節點的權重是其子結點的權重和。 OSD weight = OSD size / 1 TB CRUSH 透過重複執行Take(bucketID)和Select(n, bucketType)兩個操作選擇副本位置。兩個操作說明如下： Take(bucketID)指定從給定的 bucketID 中選擇副本位置，例如：可以指定從某台機架上選擇副本位置，以實現將不同的副本隔離在不同的故障域； Select(n, bucketType)則在給定的 Bucket 下選擇 n 個類型為 bucketType 的 Bucket，它選擇的 Bucket 主要考慮階層架構中節點的容量以及當節點新增或刪除時的資料搬移量。 算法流程 如上圖所示，為CRUSH選取副本的流程圖。其符號表示如下： bucket：Take操作指定的bucket； type：Select操作指定的 Bucket 類型； repnum：Select操作指定的副本數量； rep：當前選擇的副本編號； x：當前選擇的 PG 編號； item Bucket：代表當前被選中的 Bucket； c(r, x, in)：代表從 Bucket in 中，為 PG x 選取第 r 個副本； coolide：代表當前選中的副本位置 item 已經被選中，即出現衝突； reject：代表當前選中的副本位置 item 被拒絕，例如：item 已經處於 out 狀態的情況下； ftotal：在 Descent 區域中選擇的失敗次數，即選擇一個副本位置的總共的失敗次數； flocal：在 Local 區域中選擇的失敗次數； local_tries：在 Local 區域選擇衝突時的嘗試次數； local_fallback_tries：允許在 Local 區域的總共嘗試次數為 bucket.size + local_fallback_retries 次數，以保證遍歷完 Bucket 的所有子結點； total_tries：在 Descent 的最大嘗試次數，超過這個次數則放棄這個副本。 如上圖所示，當Take操作指定的 Bucket 和Select操作指定的 Bucket 類型之間隔著幾層的 Bucket 時，CRUSH會直接深度優先地進入到目的 Bucket 的直接父母節點。例如：從跟節點開始選擇 N 個 Host 時，它會深度優先地找到 Rack 類型的節點，並在這個節點下選取 Host 節點。為了方便描述，將 Rack 的所有子節點標記為 Local 區域；將 Take 指定的 Bucket 子結點標記為 Descent 區域。 選取過程中發生衝突、過載或者故障時，CRUSH會先在 Local 區域內重新選擇，嘗試有限次數後，如果仍然找不到滿足條件的 Bucket，那就回到 Descent 區域重新選擇。而每次重新選擇時，修改副本數目r += ftotal，所以每次選擇失敗都會遞增ftotal，因此可以盡量避免選擇時再次選到衝突的節點。","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Ceph","slug":"技術/Ceph","permalink":"http://ellis-wu.github.io/categories/技術/Ceph/"}],"tags":[{"name":"Ceph","slug":"Ceph","permalink":"http://ellis-wu.github.io/tags/Ceph/"},{"name":"CRUSH","slug":"CRUSH","permalink":"http://ellis-wu.github.io/tags/CRUSH/"}]},{"title":"持續整合工具 Jenkins 介紹與安裝","slug":"jenkins-introduction-and-install","date":"2016-09-10T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2016/09/11/jenkins-introduction-and-install/","link":"","permalink":"http://ellis-wu.github.io/2016/09/11/jenkins-introduction-and-install/","excerpt":"Jenkins 是一個持續整合 (Continuous Integration, CI)的web-based 伺服器，提供了實行持續整合的基礎設施。因為設計上擁有極高的擴充性與彈性並且為 Open Source 的專案，因此擁有為數不少開發者為其貢獻延伸套件，本身提供了許多功能如 Distrubted Builds、Test Reports、Notifications 等強大功能，近年來更廣被使用在各大專案、公司之中，例如：yahoo、facebook、NHN 以及 Sony 等。","text":"Jenkins 是一個持續整合 (Continuous Integration, CI)的web-based 伺服器，提供了實行持續整合的基礎設施。因為設計上擁有極高的擴充性與彈性並且為 Open Source 的專案，因此擁有為數不少開發者為其貢獻延伸套件，本身提供了許多功能如 Distrubted Builds、Test Reports、Notifications 等強大功能，近年來更廣被使用在各大專案、公司之中，例如：yahoo、facebook、NHN 以及 Sony 等。 再介紹 Jenkins 之前必須先簡單說明持續整合(Contunuous Integration, CI)是什麼？ 持續整合持續整合的概念最初是指在專案中由一個主要的電腦或伺服器來整合所有工程師的工作內容，並且要求每天都需要進行此一整合作業，以減少長時間累積的程式變動所帶來的影響。 簡單來說就是透過自動化工具將個模組的程式碼從版本控制伺服器上下載下來，若程式碼有所變動便進行程式碼編譯並測試，如果出問題可以馬上通知工程師進行修正。 而我們需要達到持續整合勢必需要一個工具能將平常手動的操作(像是：從版本控制伺服器下載程式碼、將程式碼編譯最後進行測試等動作)轉為自動，而 Jenkins 即是一個不錯的選擇。 Jenkins 簡介Jenkins 是一個非常老牌的持續整合工具且是一個基於MIT License的開放原始碼專案，於 2011 年一月由相同為 CI Tool 的 Hudson 專案所分支而來，原本 Hudson 名稱以及商標則為 Oracle 的管理模式，因此受到許多自由社群開發者的歡迎。 因為 Jenkins 擁有眾多使用者，讓它擁有許多 plugin 可以使用；而根據 jenkins-stats 統計，至今約有 500 多種 plugin 專來的支援，正式的 Jenkins server 安裝次數也超過 4 萬次，並有超過一百萬個 Build Jobs 運作其上。 Jeknins 安裝Jenkins 是一套由Java所開發的，它只需要Java Runtime Environment (JRE)就可以順利執行，拜 Java 跨平台的特性所賜，各大主流作業系統都能夠輕易安裝 JRE，也因此 Jenkins 能夠支援夠種主流系統。 以下將提供兩種安裝方式： GUI 安裝方式 CLI 安裝方式 GUI 安裝方式Jenkins GUI 安裝方式非常簡單，因為官方提供了非常方便的一鍵安裝檔，可以直接至 Jenkins 官方網站 下載安裝即可。 CLI 安裝方式Jenkins 官方雖然提供了非常方便的一鍵安裝，但若遇到無 GUI 介面的作業系統，即會非常的不方便，因此以下將會使用 CLI 說明如何安裝。 安裝環境環境硬體規格如下： ID Specs CPU Intel® Core™2 Quad CPU Q9400 @ 2.66GHz × 4 RAM 4 GB Disk 500 GB 而作業系統部分以下兩種作業系統版本皆測試過皆可安裝： Ubuntu 12.04 64位元 Ubuntu 14.04 64位元 因為 Jeknins 是由 Java 所撰寫，因此需要安裝 Java，而 Java 安裝將不贊述。而我測試環境的 Java 版本如下：$ java -versionjava version \"1.7.0_79\"OpenJDK Runtime Environment (IcedTea 2.5.5) (7u79-2.5.5-0ubuntu0.14.04.2)OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode) 開始安裝CLI 安裝方式也很簡單，僅需幾個指令即可：$ sudo apt-get update &amp;&amp; apt-get upgrade$ wget -q -O - https://jenkins-ci.org/debian/jenkins-ci.org.key | sudo apt-key add -$ sudo sh -c 'echo deb http://pkg.jenkins-ci.org/debian binary/ &gt; /etc/apt/sources.list.d/jenkins.list'$ sudo apt-get update$ sudo apt-get install jenkins 安裝完成後，即可打開瀏覽器http://&lt;host ip address&gt;:8080即可。 參考資料 Jenkins 官方 CI (Continuous integration) 關鍵技術：使用 Jenkins","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"DevOps","slug":"技術/DevOps","permalink":"http://ellis-wu.github.io/categories/技術/DevOps/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"http://ellis-wu.github.io/tags/Docker/"},{"name":"Jenkins","slug":"Jenkins","permalink":"http://ellis-wu.github.io/tags/Jenkins/"},{"name":"CI/CD","slug":"CI-CD","permalink":"http://ellis-wu.github.io/tags/CI-CD/"}]},{"title":"使用 HAProxy 實現負載平衡","slug":"haproxy-installation","date":"2016-04-26T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2016/04/27/haproxy-installation/","link":"","permalink":"http://ellis-wu.github.io/2016/04/27/haproxy-installation/","excerpt":"隨著網路服務的用量暴增，增加伺服器硬體設備已經無法解決問題，而為了可以擴充服務，負載平衡成為主流的技術。負載平衡除了分流能力之外，有另一個很大的好處就是可以提供高可用性(High Availability)，好讓你一台提供服務的主機失效，其他提供服務的主機可以繼續提供服務，降低斷線率。目前還使有很多大型網站都是使用典型的負載平衡架構，由一台 Switch 或 Proxy 接收來自網際網路的連線請求，然後分散給後面提供服務的伺服器叢集，藉此提高服務能處理龐大的連線數與客戶端的請求。 因此，可以透過軟體的方式來達到附載平衡的效果。因此本篇教學選用較為主流的 HAProxy 來實作負載平衡的功能。","text":"隨著網路服務的用量暴增，增加伺服器硬體設備已經無法解決問題，而為了可以擴充服務，負載平衡成為主流的技術。負載平衡除了分流能力之外，有另一個很大的好處就是可以提供高可用性(High Availability)，好讓你一台提供服務的主機失效，其他提供服務的主機可以繼續提供服務，降低斷線率。目前還使有很多大型網站都是使用典型的負載平衡架構，由一台 Switch 或 Proxy 接收來自網際網路的連線請求，然後分散給後面提供服務的伺服器叢集，藉此提高服務能處理龐大的連線數與客戶端的請求。 因此，可以透過軟體的方式來達到附載平衡的效果。因此本篇教學選用較為主流的 HAProxy 來實作負載平衡的功能。 什麼是 HAProxy ?HAPorxy 為一自由、免費、開放原始碼的軟體。是一種高效能的 Load Balance/Reverse Proxy，具有多種平衡演算法，以及具有強大狀態檢查功能，能確保客戶端(client)所送出的請求(request)導向至能正常提供服務的伺服器。其運作方式同於 L4-L7 負載平衡設備。 HAPorxy 是一個基於 TCP 與 HTTP 負載平衡軟體。其特性如下： 安全； 易於架設； 功能强大(支援 cookie track、 header rewrite 等等)； 此外系統提供簡顯易懂的系統監控以及統計報表畫面，管理者可清晰了解目前系統整體服務以及運行狀況。 HAProxy 安裝以下將會介紹HAProxy v1.5+ 在Ubuntu上的安裝方式。 安裝 HAProxy 十分容易，只需要透過apt就可以安裝完成，但在Ubuntu 14.04以後，無法安裝 HAProxy v1.5+ 的版本。因此，透過PPA新增 HAProxy v1.5+ 至apt repository 之中： $ sudo add-apt-repository ppa:vbernat/haproxy-1.5$ sudo apt-get update$ sudo apt-get install -y haproxy 若要裝 v1.6 版本，請自行將後方版本號做變更即可，指令如下：$ sudo add-apt-repository ppa:vbernat/haproxy-1.6 安裝完成後，可以使用以下指令確認版本號：$ haproxy -v HAProxy 設定編輯並加入以下資訊至/etc/haproxy/haproxy.cfg：listen www-balancer bind 0.0.0.0:9090 balance roundrobin server web1 0.0.0.0:9000 check weight 1 maxconn -1 server web2 0.0.0.0:9001 check weight 1 maxconn -1listen stats bind 0.0.0.0:8888 mode http stats enable stats hide-version stats realm Haproxy\\ Statistics stats uri / # Change your account and password stats auth &lt;account&gt;:&lt;password&gt; stats refresh 10s HAProxy 提供網頁儀表板；可以透果此網頁儀表板查看整個負載平衡群組之狀況。因此，請將設定檔中的&lt;account&gt;與&lt;password&gt;更換為自己的帳號與密碼。 設定完成後，重啟 HAProxy：$ sudo service haproxy restart 完成後，可以透過瀏覽器存取 HAProxy 儀表板。 參考資料 富人用 L4 Switch，窮人用 Linux HAProxy！","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"Linux","slug":"技術/Linux","permalink":"http://ellis-wu.github.io/categories/技術/Linux/"}],"tags":[{"name":"HAProxy","slug":"HAProxy","permalink":"http://ellis-wu.github.io/tags/HAProxy/"},{"name":"Load Balancer","slug":"Load-Balancer","permalink":"http://ellis-wu.github.io/tags/Load-Balancer/"}]},{"title":"使用 Jenkins 搭建 iOS 持續整合環境","slug":"jenkins-intergation-xctest","date":"2016-04-12T16:00:00.000Z","updated":"2019-07-22T02:21:17.797Z","comments":true,"path":"2016/04/13/jenkins-intergation-xctest/","link":"","permalink":"http://ellis-wu.github.io/2016/04/13/jenkins-intergation-xctest/","excerpt":"在 iOS 應用程式的開發中必定使用的 XCode 進行開發，且 XCode 內建的測試框架 XCTest 提供了單元測試與 UI 測試的功能。讓開發者能快速且簡易的撰寫單元測試與 UI 測試，以保障程式開發上的品質。而本文希望藉由 Jenkins 持續整合伺服器與 Gitlab 來達到自動化測試的功能，並顯示測試結果至 Jenkins 儀表板中。","text":"在 iOS 應用程式的開發中必定使用的 XCode 進行開發，且 XCode 內建的測試框架 XCTest 提供了單元測試與 UI 測試的功能。讓開發者能快速且簡易的撰寫單元測試與 UI 測試，以保障程式開發上的品質。而本文希望藉由 Jenkins 持續整合伺服器與 Gitlab 來達到自動化測試的功能，並顯示測試結果至 Jenkins 儀表板中。 而Apple在 XCode 7 中將 UI 測試加入至 XCTest 之中，為了解決開發者以前為了撰寫 UI 測試難以維護的巨大成本。而 XCTest 的 UI 測試比以往的測試工具更來得簡單方便，特別是在撰寫測試時可以使用錄製功能，大大的 UI 測試的門檻。 持續整合架構 如上圖所示，為整個系統的流程與架構。當開發者或維運人員開發完成後，將程式碼 commit/push 至 Gitlab 之中，接著 Gitlab 會觸發 Jenkins 的 Job 並開始建構本次測試，當測試完成則會顯示測試結果至 Jenkins 儀表板上。 而這邊 Jenkins 採用 Master 與 Slave 架構。Jenkins Master 為任一作業系統的機器且僅執行測試任務的分配，這是因為在公司上可能會有許多測試項目，若不採用此方法大家的測試項目可能會資源的搶佔的情形，使得測試延遲無法及時回饋給開發或維運人員，且在未來擴增測試機器也極為方便。而這邊是與 iOS 進行整合測試，所以 Jenkins Slave 使用 Mac OSX 相關設備。 事前準備 準備一台可以執行 XCode 的設備； 架設一台 Gitlab 亦可以用其他 Git 取代，但可能操作上有些許不同； 安裝 Jenkins 再任一台機器上。Jenkins 安裝可以參考 持續整合工具 Jenkins 介紹與安裝。 持續整合設定與步驟以下會一步一步進行教學，使每個人都能輕鬆的來體驗自動化測試的魅力。 Jenkins 插件安裝為了要讓開發或維運人員 commit/push 專案至版本控制伺服器時，能將版本伺服器的專案 clone 至 Jenkins Slave 上，讓專案能進行測試。因此，需要安裝Jenkins gitlab plugin。 點擊 Jenkins 儀表板右方的 【管理 Jenkins】； 接著點擊 【管理外掛程式】，並將標籤切換至 【可用的】 並搜尋 【gitlab plugin】； 選擇後開始進行安裝，安裝完成重新啟動 Jenkins。 新增 Jenkins Slave 節點讓 Jenkins 使用 Master 與 Slave 架構，將測試環境與 Jenkins CI Server 分開，好處是在於說： Jenkins CI Server 負擔不會這麼重，因為 Jenkins Build Job 都是使用記憶體，當測試一多可能會造成 Memory 不足，造成系統損毀； 同時間測試項目太多在執行，大家的測試項目會搶佔資源，可能會使得測試項目延遲，無法及時回饋給開發或維運人員； 若未來要增加測試機器，可以很方便的增加機器，若出現問題只需要針對出現問題的機器進行處理，不用整個環境重用。 透過以下步驟新增 Jenkins Slave 節點： 點擊 Jenkins 儀表板右方的 【管理 Jenkins】； 接著點擊 【管理節點】 並選擇 【新增節點】； 設定該 【節點名稱】，並選擇 【陽春 Slave】。 以上設定完成後，開始設定該節點的一些詳細資訊，而以下三個項目為 必要 的設定，其他項目依照狀況進行設定： 遠端檔案系統根目錄：此目錄為 Jenkins 遠端至 Slave 節點後，每次建構 Job 所產生的檔案都會放在此目錄之中，之後建構 Job 後產生的JUnit報表也會產生在此資料目錄底下； 標籤：設定標籤為日後建構 Job 時，指定要給該標籤的 Slave 節點建構 Job； 啟動模式：Jenkins 已經整合了幾種很方便的啟動模式，一般來講都會使用 SSH 的方式來啟動 Slave 節點，但因為 XCTest 使用 SSH 無法獲得測試結果，因此這邊選擇使用 【透過 Java Web Start 啟動 Slave 代理程式】。該方法需要在 Slave 節點上啟動一個由 Java 所撰寫而成的Jenkins slave agent。 若設定完成後 按下 【儲存】 即可。 啟動 Slave 代理程式當新增 Jenkins Slave 節點設定完成後，會在 Jenkins 儀表板中看到節點狀態為離線(Offline)，這是因為在 Slave 節點上尚未啟動Jenkins slave agent。 此時，可以點進 Slave 節點中可以看到如何啟動Jenkins slave agent，只要依照儀表板上的方法即可啟動。 在 Slave 節點上Jenkins slave agent若啟動後會跳出 connect 小視窗，這個小視窗請勿關閉，若關閉 Master 與 Slave 連線就中斷了。 安裝 ocunit2junit因為 Jenkins 看不懂 XCodebuild 後的結果，因此我們要需要透過 OCUnit2JUnit 將 Objective-C (OCUnit) test case 轉換為 Jenkins 能讀取的 JUnit 格式。 因此，在每台要執行 XCTest 的 Slave 節點上安裝 ocunit2junit。而這邊透過gem來安裝 ocunit2junit：$ sudo gem install ocunit2junit XCode 7 的 UI Test 亦可用 ocunit2junit 將結果轉換為 Jenkins 能讀取的 JUnit 格式； 若想了解更多 OCUnit2JUnit 請參考 這裡。 設定 Jenkins Job以上都完成後，開始設定 Jenkins Job。 點擊 Jenkins 儀表板右方的 【新增作業】； 設定 【作業名稱】，並選擇 【建置 Free-Style 軟體專案】。 以上設定完成後，開始設定該 Job 的詳細資訊，而以下項目為 必要 的設定，其他項目依照狀況進行設定： 限制專案執行節點設定這次工作要在哪台 Slave 節點上執行，這裡輸入設定節點時的標籤。 設定 Gitlab Repository由於事先安裝好了gitlab plugin，因此在設定 Jenkins Job 時在 【原始碼管理】 中才有 git 的選項。所以當本次 Jenkins Job 執行時會依照設定將專案 clone 下來。設定步驟如下： 設定 【Repository URL】 為自己的 Gitlab URL； 設定 【Credentials】。這邊提供了多種方法，最簡單的方式就是使用 【Username with password】，輸入自己的 Gitlab 帳號密碼即可。 這邊可以在 Branches to build 切換本次建置的 Branch。 設定建置步驟建置步驟提供了四種方式，也可以設定多個建置步驟，這邊選擇使用shell來啟動 Slave 節點上的xcodebuild。# !/bin/sh# Test a xcode project using xcodebuild command:# xcodebuild test -project &lt;projct_name&gt;.xcodeproj -scheme &lt;scheme&gt; -destination \"id=&lt;devices_id&gt;\" | ocunit2junitxcodebuild test -project DevOps_Demo.xcodeproj -scheme DevOps_Demo -destination \"id=CBE06FF7-14FD-432E-A6FD-1935E71A5DA8\" | ocunit2junit 這邊可能要注意指令中的scheme，若專案沒有scheme則無法進行測試。透過以下步驟列出專案中的scheme：$ xcodebuild -list -project &lt;projct_name&gt;.xcodeproj 若專案沒有scheme則必須於 XCode 中開啟scheme shared，開啟如下： 查詢 Devices ID 請參考 What’s your destination? 設定建置後動作根據剛剛在建置步驟設定的 shell script，當測試完成後會自動產生 JUnit 報表。因此，在建置後動作要來讀取此 JUnit 報表。而 ocunit2junit 會將報表產生在&lt;Workspace&gt;/test-reports中。 Workspace 即為新增節點時設定的遠端檔案系統根目錄。 讀取 Junit 報表步驟如下： 點選 【發佈 JUnit 測試結果報告】； 設定 【測試報告 XML】 為test-reports/*.xml。 設定 Web Hooks設定 Web Hooks 為了要讓開發或維運人員 commit/push 專案時能自動觸發 Jenkins Job 來執行本次的測試，立即測試本次的版本是否有問題出現。 首先，先取得 Jenkins Job 【馬上建置】 的網址。 接下來到 Gitlab 設定 Web Hooks，步驟如下： 選擇本次要建構自動測試的專案； 點擊 【Settings】 並點擊 【Web Hooks】； 接著將剛剛複製的 【馬上建置】 的網址貼上，並儲存即可。 之後若觸發了Trigger 條件就會執行設定的 URL，即會開始執行 Jenkins Job。 開始測試！當以上設定都完成後，就可以開始體驗自動化測試得好處。以後只要 commit/push 就會開始測試，當測試跑完，就可以在 Jenkins 儀表板上看到測試結果。","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"DevOps","slug":"技術/DevOps","permalink":"http://ellis-wu.github.io/categories/技術/DevOps/"}],"tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://ellis-wu.github.io/tags/Jenkins/"},{"name":"iOS","slug":"iOS","permalink":"http://ellis-wu.github.io/tags/iOS/"},{"name":"XCTest","slug":"XCTest","permalink":"http://ellis-wu.github.io/tags/XCTest/"},{"name":"Gitlab","slug":"Gitlab","permalink":"http://ellis-wu.github.io/tags/Gitlab/"},{"name":"CI","slug":"CI","permalink":"http://ellis-wu.github.io/tags/CI/"}]},{"title":"軟體測試方法","slug":"test-method-introduction","date":"2015-09-03T16:00:00.000Z","updated":"2019-07-22T02:21:17.798Z","comments":true,"path":"2015/09/04/test-method-introduction/","link":"","permalink":"http://ellis-wu.github.io/2015/09/04/test-method-introduction/","excerpt":"軟體專案的進行，可以囫圇吞棗，也可以鉅細彌遺，注意細節。而軟體工程的專業知識，古今中外，老生常談。但聽過是一回事，知道如何去做又是另外一回事。軟體系統不能老是屢做屢敗，而屢敗屢做！除了深耕自己的開發技能專業外，尤須注意專案管理及軟體工程的最佳實務作法。 往往一個軟體專案的失敗，可能決定於一個 bug 的產生。而為了預防這一切的發生，我們需要對軟體進行測試，而軟體就像蓋房子一樣，若蓋歪了需要馬上修正問題，否則遇到一個小小地震，就會造成整個房子倒塌。唯有持續的測試並修正，才能使房子蓋得又高又穩 ! 軟體測試方法是指測試軟體性能的方法。隨著軟體測試技術不端的發展，測試方法也越來越多樣化，針對性更強；選擇合適的軟體測試方法可以讓我們事半功倍。 而軟體測試方法有百百種，以下將介紹幾種比較常見的方法。","text":"軟體專案的進行，可以囫圇吞棗，也可以鉅細彌遺，注意細節。而軟體工程的專業知識，古今中外，老生常談。但聽過是一回事，知道如何去做又是另外一回事。軟體系統不能老是屢做屢敗，而屢敗屢做！除了深耕自己的開發技能專業外，尤須注意專案管理及軟體工程的最佳實務作法。 往往一個軟體專案的失敗，可能決定於一個 bug 的產生。而為了預防這一切的發生，我們需要對軟體進行測試，而軟體就像蓋房子一樣，若蓋歪了需要馬上修正問題，否則遇到一個小小地震，就會造成整個房子倒塌。唯有持續的測試並修正，才能使房子蓋得又高又穩 ! 軟體測試方法是指測試軟體性能的方法。隨著軟體測試技術不端的發展，測試方法也越來越多樣化，針對性更強；選擇合適的軟體測試方法可以讓我們事半功倍。 而軟體測試方法有百百種，以下將介紹幾種比較常見的方法。 測試分類 β 測試(Beta testing)： β 测试是軟體的多個用戶在一個或多個用戶的實際使用環境下進行地測試。開發者通常不在測試現場，β 測試不能由程式設計師或測試人員完成。 當開發和測試要完成所做的測試，而最終的錯誤和問題需要在最終發行前找到。這種測試一般由最終用戶或其它人員完成，不能由程式設計師或測試人員完成。 α 測試(Alpha testing)： α 測試是由一個用戶在開發環境下進行測試，也可以是公司內部的用戶在模擬實際操作環境下進行的受控測試，α 測試不能由該系統的程式設計師或測試人員完成。 再系統開發接近完成時對應用系統的測試，測試後仍然會有少量的設計變更。這種測試一般由最終用戶或其他人員來完成，不能由程式設計師或測試人員完成。 可移植性測試(Portability testing)： 又稱為兼容性測試。 可移植性測試是指測試軟體是否可以成功被移植到指定的硬體或軟體平台上。 UI 測試使用者介面測試(User interface testing)是指使用者介面的風格是否滿足客戶的要求、文字是否正確、畫面是否美觀、文字與圖片組合是否完美，操作性是否良好 ⋯⋯ 等等。UI 測試的目標是確保使用者介面會通過測試對象的功能來為使用者提供相應的訪問或瀏覽功能。確保使用者介面符合公司的標準。包括使用者的互動性、人性化、易操作性的各種測試。 分析軟體使用者介面的設計是否能達到使用者期望或要求。它常常包括 Menu、對話框以及對話框上所有的按鈕、文字、警告提示等方面的測試。 冒煙測試 冒煙測試(Smoke testing)的對象是一個新編譯的需要正視測試的軟體版本，目的是確認軟體基本功能正常，可以進行後續的正式測試工作。冒煙測試的執行者是版本編譯人員。 Example：於每日構建(Nightly build)，構建伺服器首先從CVS伺服器上，下載最新的源代碼，然後編譯單元測試，運行單元測試通過後，編譯可執行文件，可執行文件若可運行，並能執行最基本的功能，則認為通過了冒煙測試，這時，構建伺服器會把程序打包成安裝文件，然後上傳到內部網站。 第二天一早，測試人員來了以後，會收到構建伺服器發來的郵件提示昨晚是否構建成功。若構建成功，則測試人員進行相關的功能測試。 隨機測試隨機測試(Ad hoc testing)是指沒有書面資料、測試案例、期望值、檢查點 ⋯⋯ 等的測試；主要是根據使用者的經驗以及相關知識對待測軟體進行功能或效能抽查，有時會包括到一些當前測試案例沒有覆蓋到的部分，另外，也可以針對軟體新增或更新的功能做一些重點測試，如特殊場景、特殊環境，並可以結合回歸測試一起進行。 Example： 我來試試看如果當聽音樂時打電話進來會發生什麼事？ 任意調整視窗大小 白箱測試與黑箱測試 白箱測試白箱測試(White box testing)也稱為結構性測試(Structural testing)、透明盒測試(Glass box testing)。是以測試的深度為主，可分為資料流程面(Data Flow Coverage)以及控制流程面(Control Flow Coverage)兩個層面： 資料流程面：測試資料在程式內所經過的流程。 控制流程面：測試程式在執行過程中每個階段的流程。 白箱測試時，測試人員必須掌握程式原始碼，瞭解其內部運作，包含資料流程以及程式控制流程。 黑箱測試黑箱測試(Black box testing)則不需要對軟體的內部結構有深層的了解，直接就功能面來驗證，所以在程式碼的安全性檢測方面，黑箱測試就是所謂的「動態程式碼安全性檢測」，其中「動態」是指應用程式處於執行期的狀態，黑箱測試的手法則是模擬駭客的攻擊，測試系統有沒有漏洞。而白箱就是「靜態程式碼安全性檢測」，它直接掃描程式寫法。黑箱測試使用已知的攻擊樣板 (Pattern) 進行檢測。 黑箱測試有以下幾個原則： 測試應用程式的功能，而不是其內部結構運作。 測試者只需要知道什麼是系統應該做的事，即當鍵入一個特定的輸入，可以得到一定的輸出。 測試者選擇有效輸入和無效輸入來驗證是否正確的輸出。 白箱與黑箱測試優缺點分析 自動化測試使用自動化測試工具來進行測試，這類測試一般不需要人來干預，通常在 GUI 測試、效能測試、功能測試中用的較多。透過錄製測試腳本，然後執行這個測試腳本來實現測試過程的自動化。 回歸測試回歸測試(Regression testing)是指在發生修改之後重新測試先前的測試以保證修改的正確性。理論上，軟體產生新版本，都需要進行回歸測試，驗證以前發現和修復的錯誤是否在新版本上再次出現。 驗收測試驗收測試(Acceptance testing)是指 系統開發生命週期 方法論的一個階段，這時相關的使用者或獨立的測試人員根據測試計劃和結果對系統進行測試和接收。它讓系統使用者決定是否接收系統。它是一項確定產品是否能夠滿足合約或使用者所規定需求的測試。 單元測試單元測試(Unit testing)是最小規模的測試；以測試某個功能或部分代碼。由程式設計師來進行測試而非測試員來做，因為它需要知道內部程式設計和編碼的細節。 單元測試是最常見的測試，在Open Source中為最常見的測試項目之一。其驗證開發者的程式碼邏輯及判斷是否正確。 整合測試整合測試(Integration testing)是單元測試的邏輯延伸。在最簡單的形式中，已經測試完成的兩個單元會結合成一個元件，而且會測試它們之間的介面。在這方面，元件是指多個單元的整合彙總(Aggregate)。在現實狀況下，許多單元會結合成元件，依序再彙總(Aggregate) 成程式中較大的部分。這個想法是測試片段的組合，最後擴展此程序，以測試您的模組與其他群組的模組。最後一起測試構成程序的所有模組。此外，如果程式包含多個程序，就應該成對進行測試，而不是一次完成測試。 整合測試可識別組合單元時所發生的問題。藉由使用需要測試各單元以便在組合各單元之前確定其可行性的測試規劃，可以了解，組合各單元時所發現的任何錯誤都可能與單元之間的介面有關。這個方法可大為減少其可能性，使分析變得比較簡單。 端到端測試端到端測試(End to End testing)亦可稱為 E2E 測試。簡單來說，是從使用者與應用程式的互動方式來進行測試，因此寫測試時，會從操作的面向來思考測試寫法。而端到端測試包含所有訪問點的功能測試及效能測試。端到端測試實質上是一種「灰盒」測試，一種集合了白盒測試與黑盒測試優點的測試方法。 壓力/負載/效能測試以下先簡單說明三種測試方法的定義： 效能測試：屬於在正常的執行下，確認各個數據有符合期望值； 負載測試：在瀕臨上限時還是可以正常執行，目的在測試是否有 memory leak 問題； 壓力測試：在超過上限後是否正常執行或 crash 之後是否可以回復之前的狀態。 效能測試效能測試(Performance testing)的目的不是要找缺陷(bug)，而是要解決瓶頸和替未來的回歸測試(regression testing)建立一個底線。效能測試是為獲取或驗證系統效能指標而進行的測試，多數情況下效能測試會在不同負載情況下進行。 效能測試主要目的在系統的效能是否能符合客戶的需求，因此執行效能測試時，首先需要瞭解客戶的需求為何。 負載測試負載測試(Load testing)是系統所能承受的條件下，逐漸的增加負載來觀察不同負載下系統的回應時間、數據的吞吐量和系統佔用資源(例如：CPU、記憶體) ⋯⋯ 等等，以檢驗系統的行為和特性，藉此發現系統可能存在的效能瓶頸以及記憶體洩漏 ⋯⋯ 等等。 在測試文獻中，負載測試一詞經常被定義為提供系統所能執行最大工作量下運作測試的流程，負載測試通常被稱為「容量測試」或「壽命(longevity)/耐力(endurance)測試」。 容量測試的例子： 藉由修改一個非常大的檔案以測試一個文字處理器。 藉由上千位使用者的收件匣以測試一個郵件伺服器。 一個容量測試的特定 Case 是 0 容量測試，已確認系統能接受空的工作。 壽命/耐力測試的例子： 在 Client 端執行一個 Client-Server 應用程式以考驗伺服器在延長一段時間下的狀況。 壓力測試壓力測試是對資統資源不斷施加壓力(超出系統資源或拿走系統資源)，觀察系統最後在什麼樣的壓力下或系統哪部分被壓垮，以及當系統被壓垮後是否能平順的結束與恢復，因此主要目的是在確保系統可以平順的結束與恢復。 Ron Ratton 的 《Software Testing》 中對壓力測試的定義為一種破壞性的測試，故意讓系統在比較少的資源環境下執行(例如：較低的記憶體、硬碟空間較少、較高的 CPU 使用率 ⋯⋯ 等等)，執行至系統無法被壓垮後從而發現系統的缺陷。用一句話來比喻就是讓系統載飢餓的狀態下執行。 壓力測試又可以分為以下兩種： 穩定壓力測試：在特定壓力下，長時間一直進行，查看是否有OOM(out of memory)或是系統會不會反應越來越慢； 破壞性壓力測試：透過不斷的加壓可以讓原本穩定壓力測試的問題更快的發生。 而壓力測試可以分為以下三種方向： Spike Testing：短時間的極端負載測試。例如：在搶購江蕙演唱會門票的時候，在某個時間突然湧入大量用戶搶購； Extreme testing：在過量用戶下的負載測試。例如：某網站最多只允許 100 人同時上現操作，卻給它 110 人； Hammer testing：連續執行所有能做的操作。 邊界值分析邊界值分析(Boundary Value Analysis, BVA)從過往經驗來看，許多問題都發生在輸入或輸出範圍的邊界值上，而不是在輸入範圍內，因此，我們必須小心去分析邊界值條件，針對邊界值去設計測試案例，以免忽略邊界值的錯誤。 BVA 是一種黑箱測試的方法。特別適用偵測以下幾種錯誤： 人為造成的錯誤資料型態 指定錯誤關係運算子 資料型態的封裝 迴圈結構的問題 差一錯誤 (off-by-one-errors) 隱藏的邊界值：並不是所有的邊界值都可以用數值表示，或是直接讓使用者觀察到，像是程式中會有許多迴圈，或是一些資料型態，也都存在邊界值的問題。 Example：輸入一位學生軟體工程成績，成績範圍：0 ~ 100(可輸入最小值為0，可輸入最大值為100)。所以根據邊界值分析，我們會取-1, 0, 1, 99, 100, 101 來做邊界值測試，之後會再取範圍內的任一值(假設我們取 60)當作確認輸入範圍內的直也為正常。 結果如下： No. 成績 預期輸出 1 -1 跳出錯誤訊息 2 0 成績數值正常 3 1 成績數值正常 4 99 成績數值正常 5 100 成績數值正常 6 101 成績數值正常 7 [0-100] 成績數值正常 總結測試的方法不勝枚舉，而以上都是我目前從專案中學習到的一些測試方法。或許許多人都會覺得測試是讓費時間，如果你不想要遇到軟體給廠商後發現一堆問題，才開始後悔當初應該好好做測試這類情況發生的話，就開始動手學習如何測試吧！ 參考文獻 軟體測試方法 台灣WiKi - 冒煙測試 JUF學習紀錄本 - 「黑箱」與「白箱」測試的區別 台灣Wiki - 驗收測試 整合測試 - MSDN - Microsoft 科科和測試 - 邊界值分析 ( Boundary Value Analysis ; BVA )","categories":[{"name":"技術","slug":"技術","permalink":"http://ellis-wu.github.io/categories/技術/"},{"name":"DevOps","slug":"技術/DevOps","permalink":"http://ellis-wu.github.io/categories/技術/DevOps/"}],"tags":[{"name":"Testing","slug":"Testing","permalink":"http://ellis-wu.github.io/tags/Testing/"}]}]}